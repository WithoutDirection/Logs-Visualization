{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3218169c",
   "metadata": {},
   "source": [
    "# Êó•Ë™åË¶ñË¶∫ÂåñËàáÊîªÊìäÂ∫èÂàóÂàÜÊûêÁ≥ªÁµ± (Log Visualization and Attack Sequence Analysis System)\n",
    "\n",
    "Êú¨Â∞àÊ°àÁî®ÊñºËôïÁêÜ Caldera ÊîªÊìäÊ®°Êì¨Âπ≥Âè∞Áî¢ÁîüÁöÑÊó•Ë™åË≥áÊñôÔºåÂ∞áÂÖ∂ËΩâÊèõÁÇ∫Á∂≤Ë∑ØÂúñ(Graph)‰∏¶ÈÄ≤Ë°åË¶ñË¶∫ÂåñÂàÜÊûê„ÄÇ‰∏ªË¶ÅÂäüËÉΩÂåÖÊã¨Ôºö\n",
    "\n",
    "1. **Ë≥áÊñôÈ†êËôïÁêÜ**: Â∞á CSV Ê†ºÂºèÁöÑÊó•Ë™åÊ™îÊ°àËΩâÊèõÁÇ∫Ê®ôÊ∫ñÂåñÊ†ºÂºè\n",
    "2. **ÂúñÂΩ¢Âª∫Êßã**: Âü∫ÊñºÊó•Ë™å‰∫ã‰ª∂Âª∫Á´ãÁØÄÈªûÂíåÈÇäÁöÑÈóú‰øÇÂúñ\n",
    "3. **ÊîªÊìäÂ∫èÂàóÂÅµÊ∏¨**: Ë≠òÂà•È†êÂÆöÁæ©ÁöÑÊîªÊìäÊ®°ÂºèÂíåË°åÁÇ∫Â∫èÂàó\n",
    "4. **‰∫íÂãïÂºèË¶ñË¶∫Âåñ**: Áî¢ÁîüÂèØ‰∫íÂãïÁöÑ HTML ÂúñÂΩ¢‰ªãÈù¢ÔºåÊîØÊè¥ÊôÇÈñìÁØ©ÈÅ∏ÂíåÂ∫èÂàóÂàÜÊûê\n",
    "\n",
    "## Á≥ªÁµ±Êû∂Êßã (System Architecture)\n",
    "- **Ëº∏ÂÖ•**: Caldera_Ability_Statistics/ ÁõÆÈåÑ‰∏≠ÁöÑ CSV Êó•Ë™åÊ™îÊ°à(Áî± Procmon ÈåÑË£Ω)\n",
    "- **ËôïÁêÜ**: ‰ΩøÁî® NetworkX Âª∫Á´ãÂúñÂΩ¢ÁµêÊßãÔºå‰∏¶ÊáâÁî®ÊîªÊìäÊ®°ÂºèÊØîÂ∞çÊºîÁÆóÊ≥ï\n",
    "- **Ëº∏Âá∫**: ÂÑ≤Â≠òÁÇ∫ pickle Ê†ºÂºèÁöÑÂúñÂΩ¢Ê™îÊ°àÂíå‰∫íÂãïÂºè HTML Ë¶ñË¶∫Âåñ‰ªãÈù¢"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fa00b",
   "metadata": {},
   "source": [
    "# Â•ó‰ª∂ÂåØÂÖ•ËàáÂàùÂßãË®≠ÂÆö (Package Imports and Initial Configuration)\n",
    "\n",
    "Ê≠§ÂñÆÂÖÉÊ†ºÂåØÂÖ•ÊâÄÊúâÂøÖË¶ÅÁöÑ Python Â•ó‰ª∂ÂíåÊ®°ÁµÑÔºö\n",
    "\n",
    "## Ê†∏ÂøÉÂ•ó‰ª∂ (Core Packages)\n",
    "- **glob, os**: Ê™îÊ°àÁ≥ªÁµ±Êìç‰ΩúÂíåË∑ØÂæëËôïÁêÜ\n",
    "- **networkx**: ÂúñÂΩ¢Ë≥áÊñôÁµêÊßãÁöÑÂª∫Á´ãËàáÊìç‰Ωú\n",
    "- **pandas**: Ë≥áÊñôËôïÁêÜÂíå CSV Ê™îÊ°àËÆÄÂèñ\n",
    "- **matplotlib**: Âü∫Á§éÂúñÂΩ¢Áπ™Ë£ΩÂäüËÉΩ\n",
    "\n",
    "## Â∞àÁî®Â∑•ÂÖ∑ (Specialized Tools)\n",
    "- **tqdm**: ÈÄ≤Â∫¶Ê¢ùÈ°ØÁ§∫ÔºåÁî®ÊñºÈï∑ÊôÇÈñìÈÅãË°åÁöÑËø¥Âúà\n",
    "- **natsort**: Ëá™ÁÑ∂ÊéíÂ∫èÔºåÁ¢∫‰øùÊ™îÊ°àÊåâÊ≠£Á¢∫È†ÜÂ∫èËôïÁêÜ\n",
    "- **pickle**: Áâ©‰ª∂Â∫èÂàóÂåñÔºåÁî®ÊñºÂÑ≤Â≠òÂúñÂΩ¢ÁµêÊßã\n",
    "- **uuid**: Áî¢ÁîüÂîØ‰∏ÄË≠òÂà•Á¢ºÔºåÁî®ÊñºÁØÄÈªûÊ®ôË≠ò\n",
    "\n",
    "## Ëá™ÂÆöÁæ©Ê®°ÁµÑ (Custom Modules)\n",
    "- **graphutil**: ÂåÖÂê´ÂúñÂΩ¢ÁîüÊàêÂíåË¶ñË¶∫ÂåñÁöÑÊ†∏ÂøÉÂáΩÊï∏\n",
    "\n",
    "Ë®≠ÂÆöËº∏Âá∫ÁõÆÈåÑ GRAPH_PATH = \"Graphs\" Áî®ÊñºÂÑ≤Â≠òËôïÁêÜÂæåÁöÑÂúñÂΩ¢Ê™îÊ°à„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d4ce0",
   "metadata": {},
   "source": [
    "# Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from natsort import natsorted\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "from graphutil import *\n",
    "\n",
    "GRAPH_PATH = \"Graphs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3abc1",
   "metadata": {},
   "source": [
    "# ÊîªÊìäÂ∫èÂàóÊ®°ÂºèÂÆöÁæ© (Attack Sequence Pattern Definition)\n",
    "\n",
    "Êú¨ÂñÆÂÖÉÊ†ºÂÆöÁæ©‰∫ÜÁ≥ªÁµ±Áî®ÊñºË≠òÂà•ÊîªÊìäË°åÁÇ∫ÁöÑÈ†êË®≠Ê®°Âºè„ÄÇÊØèÂÄãÊ®°Âºè‰ΩøÁî® `SequencePattern` ÂëΩÂêçÂÖÉÁµÑ(namedtuple)ÁµêÊßãÔºåÂåÖÂê´‰ª•‰∏ãÂ±¨ÊÄßÔºö\n",
    "\n",
    "## Ê®°ÂºèÂ±¨ÊÄßË™™Êòé (Pattern Attributes)\n",
    "- **name**: Ê®°ÂºèÂêçÁ®±ÔºåÁî®ÊñºË≠òÂà•ÊîªÊìäÈ°ûÂûã\n",
    "- **operations**: Êìç‰ΩúÂ∫èÂàóÂàóË°®ÔºåÂÆöÁæ©Ë©≤ÊîªÊìäÊ®°ÂºèÂåÖÂê´ÁöÑÁ≥ªÁµ±Êìç‰Ωú\n",
    "- **color**: Ë¶ñË¶∫ÂåñÊôÇ‰ΩøÁî®ÁöÑÈ°èËâ≤‰ª£Á¢º\n",
    "- **description**: Ê®°ÂºèÁöÑÊñáÂ≠óÊèèËø∞\n",
    "- **min_length**: ÊúÄÂ∞èÂ∫èÂàóÈï∑Â∫¶Ë¶ÅÊ±Ç\n",
    "- **strict_order**: ÊòØÂê¶Ë¶ÅÊ±ÇÂö¥Ê†ºÁöÑÊìç‰ΩúÈ†ÜÂ∫è\n",
    "- **results**: È†êÊúüÁöÑÊìç‰ΩúÁµêÊûúÁãÄÊÖã\n",
    "\n",
    "## È†êÂÆöÁæ©ÊîªÊìäÊ®°Âºè (Predefined Attack Patterns)\n",
    "\n",
    "### 1. Á®ãÂ∫èÂâµÂª∫Ê®°Âºè (Process Creation)\n",
    "- ÂÅµÊ∏¨Êñ∞Á®ãÂ∫èÁöÑÂâµÂª∫Ë°åÁÇ∫\n",
    "- È°èËâ≤ÔºöÁ¥ÖËâ≤ (#FF3333)\n",
    "\n",
    "### 2. Ê™îÊ°àÊìç‰ΩúÊ®°Âºè (File Manipulation)\n",
    "- **File_Creation_Write**: Ê™îÊ°àÂâµÂª∫ÂæåÈÄ≤Ë°åÂØ´ÂÖ•\n",
    "- **File_Creation_Metadata_Write**: ÂåÖÂê´ÂÖÉË≥áÊñôÊü•Ë©¢ÁöÑÂÆåÊï¥Ê™îÊ°àÊìç‰Ωú\n",
    "- È°èËâ≤ÔºöÊ©ôËâ≤/ÈªÉËâ≤Á≥ª\n",
    "\n",
    "### 3. Ë®ªÂÜäË°®Êìç‰ΩúÊ®°Âºè (Registry Manipulation)\n",
    "- **Registry_Creation_Modification**: Ë®ªÂÜäË°®ÈçµÂÄºÁöÑÂâµÂª∫Âíå‰øÆÊîπ\n",
    "- **Registry_Modification**: ÁèæÊúâË®ªÂÜäË°®ÈçµÂÄºÁöÑ‰øÆÊîπ\n",
    "- È°èËâ≤ÔºöÈùíËâ≤/Á∂†Ëâ≤Á≥ª\n",
    "\n",
    "### 4. Á∂≤Ë∑ØÈÄöË®äÊ®°Âºè (Network Communication)\n",
    "- **TCP_Communication**: TCP ÈÄ£Êé•ÁöÑÂÆåÊï¥ÁîüÂëΩÈÄ±Êúü\n",
    "- ÂåÖÂê´ÈÄ£Êé•„ÄÅÁôºÈÄÅ„ÄÅÊé•Êî∂„ÄÅÊñ∑ÈñãÁ≠âÊ≠•È©ü\n",
    "- È°èËâ≤ÔºöËóçËâ≤ (#0066FF)\n",
    "\n",
    "ÈÄô‰∫õÊ®°ÂºèÂ∞áÂú®ÂæåÁ∫åÁöÑÂúñÂΩ¢ÂàÜÊûê‰∏≠Áî®ÊñºËá™ÂãïË≠òÂà•ÂíåÊ®ôË®òÂèØÁñëÁöÑÊîªÊìäË°åÁÇ∫Â∫èÂàó„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d4bc9",
   "metadata": {},
   "source": [
    "# Rules for the sequence matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined attack sequence patterns based on exact operation names\n",
    "SequencePattern = namedtuple('SequencePattern', ['name', 'operations', 'color', 'description', 'min_length', 'strict_order', 'results'])\n",
    "\n",
    "\n",
    "ATTACK_SEQUENCE_PATTERNS = [\n",
    "    # Process creation patterns\n",
    "    SequencePattern(\n",
    "        name=\"Process_Creation\",\n",
    "        operations=[\"Process Create\"],\n",
    "        color=\"#FF3333\",  # Red\n",
    "        description=\"Process creation operations\",\n",
    "        min_length=1,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\"]  # Expected results for this pattern\n",
    "    ),\n",
    "\n",
    "    # File manipulation patterns\n",
    "    SequencePattern(\n",
    "        name=\"File_Creation_Write\",\n",
    "        operations=[\"CreateFile\", \"WriteFile\"],\n",
    "        color=\"#FF6600\",  # Orange\n",
    "        description=\"File creation followed by write operations\",\n",
    "        min_length=2,\n",
    "        strict_order=True,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "    SequencePattern(\n",
    "        name=\"File_Creation_Metadata_Write\",\n",
    "        operations=[\"CreateFile\", \"QueryBasicInformationFile\", \"WriteFile\", \"CloseFile\"],\n",
    "        color=\"#FAD000\",  # Yellow\n",
    "        description=\"File creation with metadata query and write\",\n",
    "        min_length=1,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    # Registry manipulation patterns\n",
    "    SequencePattern(\n",
    "        name=\"Registry_Creation_Modification\",\n",
    "        operations=[\"RegCreateKey\", \"RegSetValue\", \"RegQueryKey\", \"RegCloseKey\"],\n",
    "        color=\"#34CCFF\",  # Cyan\n",
    "        description=\"Registry key Create and modification\",\n",
    "        min_length=2,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    SequencePattern(\n",
    "        name=\"Registry_Modification\",\n",
    "        operations=[\"RegOpenKey\", \"RegSetValue\", \"RegQueryKey\", \"RegCloseKey\"],\n",
    "        color=\"#33CC33\",  # Green\n",
    "        description=\"Registry key open and modification\",\n",
    "        min_length=2,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    # Network communication patterns\n",
    "    SequencePattern(\n",
    "        name=\"TCP_Communication\",\n",
    "        operations=[\"TCP Connect\", \"TCP Send\", \"TCP Receive\", \"TCP Disconnect\"],\n",
    "        color=\"#0066FF\",  # Blue\n",
    "        description=\"TCP network communication sequence\",\n",
    "        min_length=2,\n",
    "        strict_order=True,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"] \n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec388f28",
   "metadata": {},
   "source": [
    "# Êó•Ë™åÊ™îÊ°àËΩâÊèõÁÇ∫ÂúñÂΩ¢ÁµêÊßã (Convert Log Files to Graph Structure)\n",
    "\n",
    "Ê≠§ÂçÄÂ°äË≤†Ë≤¨Â∞á CSV Ê†ºÂºèÁöÑÊó•Ë™åÊ™îÊ°àËΩâÊèõÁÇ∫Á∂≤Ë∑ØÂúñ(NetworkX Graph)ÁµêÊßãÔºåÊòØÊï¥ÂÄãÁ≥ªÁµ±ÁöÑÊ†∏ÂøÉË≥áÊñôËôïÁêÜÊ≠•È©ü„ÄÇ\n",
    "\n",
    "## ‰∏ªË¶ÅËôïÁêÜÊµÅÁ®ã (Main Processing Flow)\n",
    "\n",
    "### 1. Ê™îÊ°àÊé¢Á¥¢ËàáÈ©óË≠â (File Discovery and Validation)\n",
    "- ÊêúÂ∞ã `Caldera_Ability_Statistics/` ÁõÆÈåÑ‰∏≠ÁöÑ CSV Ê™îÊ°à\n",
    "- ÂÑ™ÂÖàËôïÁêÜÂ∑≤Êúâ `_raw_events_with_lineid.csv` ÂæåÁ∂¥ÁöÑÊ™îÊ°à\n",
    "- Ëã•‰∏çÂ≠òÂú®ÔºåÂâáËá™ÂãïÁÇ∫ÂéüÂßã CSV Ê™îÊ°àÊ∑ªÂä† `lineid` Ê¨Ñ‰Ωç\n",
    "\n",
    "### 2. Á∑®Á¢ºËôïÁêÜ (Encoding Handling)\n",
    "Á≥ªÁµ±ÂòóË©¶Â§öÁ®ÆÂ≠óÂÖÉÁ∑®Á¢º‰ª•Á¢∫‰øùÁõ∏ÂÆπÊÄßÔºö\n",
    "- UTF-8 (Ê®ôÊ∫ñÁ∑®Á¢º)\n",
    "- UTF-8-BOM (ÂåÖÂê´‰ΩçÂÖÉÁµÑÈ†ÜÂ∫èÊ®ôË®ò)\n",
    "- CP950 (ÁπÅÈ´î‰∏≠Êñá Big5 Á∑®Á¢º)\n",
    "\n",
    "### 3. Ë≥áÊñôÁµêÊßãËΩâÊèõ (Data Structure Conversion)\n",
    "Â∞á CSV Ë≥áÊñôÂàóËΩâÊèõÁÇ∫ `campaign_events` Ê†ºÂºèÔºåÂåÖÂê´Ôºö\n",
    "- **srcNode**: ‰æÜÊ∫êÁØÄÈªûË≥áË®ä (ÈÄöÂ∏∏ÁÇ∫Á®ãÂ∫è)\n",
    "- **dstNode**: ÁõÆÊ®ôÁØÄÈªûË≥áË®ä (Ê™îÊ°à„ÄÅË®ªÂÜäË°®„ÄÅÁ∂≤Ë∑ØÁ≠â)\n",
    "- **relation**: Êìç‰ΩúÈóú‰øÇÈ°ûÂûã\n",
    "- **timestamp**: ÊôÇÈñìÊà≥Ë®ò\n",
    "- **line_id**: ÂéüÂßãÊó•Ë™åË°åËôü\n",
    "\n",
    "### 4. ÁØÄÈªûÈ°ûÂûãÂàÜÈ°û (Node Type Classification)\n",
    "Ê†πÊìö‰∫ã‰ª∂È°ûÂà•Ëá™ÂãïÂàÜÈ°ûÁØÄÈªûÔºö\n",
    "- **Process**: Á®ãÂ∫èÁØÄÈªûÔºåÂåÖÂê´ PID ÂíåÁ®ãÂ∫èÂêçÁ®±\n",
    "- **File**: Ê™îÊ°àÁ≥ªÁµ±ÁØÄÈªûÔºåÂåÖÂê´Ë∑ØÂæëË≥áË®ä\n",
    "- **Registry**: Ë®ªÂÜäË°®ÁØÄÈªûÔºåÂåÖÂê´ÈçµÂÄºË∑ØÂæë\n",
    "- **Network**: Á∂≤Ë∑ØÁØÄÈªûÔºåÂåÖÂê´ IP ‰ΩçÂùÄË≥áË®ä\n",
    "\n",
    "### 5. UUID ÁîüÊàêÁ≠ñÁï• (UUID Generation Strategy)\n",
    "‰ΩøÁî® UUID5 (Âü∫ÊñºÂëΩÂêçÁ©∫ÈñìÁöÑÈõúÊπä) Á¢∫‰øùÁõ∏ÂêåË≥áÊ∫êÁöÑÂîØ‰∏ÄÊÄßÔºö\n",
    "- Á®ãÂ∫è: `Á®ãÂ∫èÂêçÁ®±_PID`\n",
    "- Ê™îÊ°à: `Ê™îÊ°àË∑ØÂæë`\n",
    "- Ë®ªÂÜäË°®: `Ë®ªÂÜäË°®ÈçµÂÄºË∑ØÂæë`\n",
    "- Á∂≤Ë∑Ø: `IP‰ΩçÂùÄ`\n",
    "\n",
    "ÈÄôÁ®ÆÊñπÊ≥ïÁ¢∫‰øù‰∫ÜÂúñÂΩ¢‰∏≠Áõ∏ÂêåË≥áÊ∫êÁöÑÁØÄÈªûÊúÉË¢´Ê≠£Á¢∫Âêà‰ΩµÔºåÈÅøÂÖçÈáçË§áÁØÄÈªûÁöÑÁî¢Áîü„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45c98e",
   "metadata": {},
   "source": [
    "# Convert all logs into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caldera_dir = os.path.join(os.getcwd(), 'Caldera_Ability_Statistics')\n",
    "# for file in os.listdir(caldera_dir):\n",
    "#     if file.endswith(\"_raw_events_with_lineid.csv\"):\n",
    "#         # remove the old processed files\n",
    "#         os.remove(os.path.join(caldera_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263895f",
   "metadata": {},
   "source": [
    "# Ê™îÊ°àÊ∫ñÂÇôËàáÈ†êËôïÁêÜ (File Preparation and Preprocessing)\n",
    "\n",
    "Ê≠§ÂçÄÂ°äË≤†Ë≤¨Ê∫ñÂÇôÂíåÈ†êËôïÁêÜËº∏ÂÖ•ÁöÑ CSV Êó•Ë™åÊ™îÊ°àÔºåÁ¢∫‰øùË≥áÊñôÊ†ºÂºèÁöÑÁµ±‰∏ÄÊÄß„ÄÇ\n",
    "\n",
    "## ËôïÁêÜÈÇèËºØ (Processing Logic)\n",
    "\n",
    "### 1. Ê™îÊ°àÊêúÂ∞ãÁ≠ñÁï• (File Search Strategy)\n",
    "- **ÂÑ™ÂÖàÁ¥ö 1**: Â∞ãÊâæÂ∑≤Â≠òÂú®ÁöÑ `*_raw_events_with_lineid.csv` Ê™îÊ°à\n",
    "- **ÂÑ™ÂÖàÁ¥ö 2**: Ëã•ÁÑ°È†êËôïÁêÜÊ™îÊ°àÔºåÂâáËôïÁêÜÂéüÂßã CSV Ê™îÊ°à‰∏¶Ê∑ªÂä†Ë°åËôü\n",
    "\n",
    "### 2. Ëá™ÂãïÊ†ºÂºèËΩâÊèõ (Automatic Format Conversion)\n",
    "Áï∂Êâæ‰∏çÂà∞È†êËôïÁêÜÊ™îÊ°àÊôÇÔºåÁ≥ªÁµ±ÊúÉÔºö\n",
    "- ÊéÉÊèè `Caldera_Ability_Statistics/` ÁõÆÈåÑ\n",
    "- ÁÇ∫ÊØèÂÄãÂéüÂßã CSV Ê™îÊ°àÊ∑ªÂä† `lineid` Ê¨Ñ‰Ωç\n",
    "- ËôïÁêÜÂ§öÁ®ÆÂ≠óÂÖÉÁ∑®Á¢ºÂïèÈ°å (UTF-8, UTF-8-BOM, CP1252, ISO-8859-1, CP950)\n",
    "- Â∞áËΩâÊèõÂæåÁöÑÊ™îÊ°àÂÑ≤Â≠òÁÇ∫ `*_raw_events_with_lineid.csv` Ê†ºÂºè\n",
    "\n",
    "### 3. ÈåØË™§ËôïÁêÜÊ©üÂà∂ (Error Handling)\n",
    "- Ëá™ÂãïÂòóË©¶Â§öÁ®ÆÁ∑®Á¢ºÊ†ºÂºè\n",
    "- Ë∑≥ÈÅéÁÑ°Ê≥ïËß£Á¢ºÁöÑÊ™îÊ°à‰∏¶Ë®òÈåÑÈåØË™§\n",
    "- Êèê‰æõË©≥Á¥∞ÁöÑËôïÁêÜÈÄ≤Â∫¶ÂõûÈ•ã\n",
    "\n",
    "### 4. Ë≥áÊñôÂÆåÊï¥ÊÄßÊ™¢Êü• (Data Integrity Check)\n",
    "- È©óË≠âÂøÖË¶ÅÊ¨Ñ‰ΩçÁöÑÂ≠òÂú®ÊÄß\n",
    "- Ëá™ÂãïÊ∑ªÂä†Áº∫Â§±ÁöÑ `lineid` Ê¨Ñ‰Ωç\n",
    "- Á¢∫‰øùË≥áÊñôÊ†ºÂºèÁ¨¶ÂêàÂæåÁ∫åËôïÁêÜÈúÄÊ±Ç\n",
    "\n",
    "ÈÄôÂÄãÈ†êËôïÁêÜÊ≠•È©üÁ¢∫‰øù‰∫ÜÊâÄÊúâËº∏ÂÖ•Ê™îÊ°àÈÉΩÂÖ∑ÊúâÁµ±‰∏ÄÁöÑÊ†ºÂºèÔºåÂåÖÂê´ÂøÖË¶ÅÁöÑË°åËôüË≥áË®äÔºåÁÇ∫ÂæåÁ∫åÁöÑÂúñÂΩ¢Âª∫ÊßãÊèê‰æõÂèØÈù†ÁöÑË≥áÊñôÂü∫Á§é„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11064976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for CSV files that match the attached dataset pattern\n",
    "csv_files = natsorted(glob(os.path.join(os.getcwd(), 'Caldera_Ability_Statistics/*_raw_events_with_lineid.csv')))\n",
    "if csv_files:\n",
    "    files = csv_files\n",
    "    print(f\"Found existing *_raw_events_with_lineid.csv files: {len(files)}\")\n",
    "else:\n",
    "    # create *_raw_events_with_lineid.csv list from directory\n",
    "    print(\"No *_raw_events_with_lineid.csv files found. Converting existing CSV files...\")\n",
    "    files = []\n",
    "    \n",
    "    caldera_dir = os.path.join(os.getcwd(), 'Caldera_Ability_Statistics')\n",
    "    if not os.path.exists(caldera_dir):\n",
    "        os.makedirs(caldera_dir)\n",
    "        print(f\"Created directory: {caldera_dir}\")\n",
    "        print(\"Please place your CSV files in this directory and run again.\")\n",
    "    else:\n",
    "        for file in os.listdir(caldera_dir):\n",
    "            if file.endswith(\".csv\") and not file.endswith(\"_raw_events_with_lineid.csv\"):\n",
    "                original_file = os.path.join(caldera_dir, file)\n",
    "                \n",
    "                # Read the original CSV with encoding handling\n",
    "                try:\n",
    "                    # Try different encodings\n",
    "                    encodings_to_try = ['utf-8', 'utf-8-sig', 'cp1252', 'iso-8859-1', 'cp950']\n",
    "                    df = None\n",
    "                    \n",
    "                    for encoding in encodings_to_try:\n",
    "                        try:\n",
    "                            df = pd.read_csv(original_file, encoding=encoding)\n",
    "                            break\n",
    "                        except UnicodeDecodeError:\n",
    "                            continue\n",
    "                    \n",
    "                    if df is None:\n",
    "                        print(f\"  ‚ùå Could not decode {file} with any supported encoding\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add lineid column if it doesn't exist\n",
    "                    if 'lineid' not in df.columns and 'LineID' not in df.columns and 'line_id' not in df.columns:\n",
    "                        df['lineid'] = range(1, len(df) + 1)\n",
    "                        # print(f\"  Added lineid column to {file}\")\n",
    "                    \n",
    "                    # Generate new filename\n",
    "                    base_name = file.replace('.csv', '')\n",
    "                    new_filename = f\"{base_name}_raw_events_with_lineid.csv\"\n",
    "                    new_filepath = os.path.join(caldera_dir, new_filename)\n",
    "                    \n",
    "                    # Save the modified CSV with UTF-8 encoding\n",
    "                    df.to_csv(new_filepath, index=False, encoding='utf-8')\n",
    "                    files.append(new_filepath)\n",
    "                    print(f\"\\r‚úÖ Created: {new_filename}\", end='', flush=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\r‚ùå Error processing {file}: {e}\", end='', flush=True)\n",
    "                    continue\n",
    "        \n",
    "        if not files:\n",
    "            print(\"\\nNo CSV files found to convert. Please add CSV files to Caldera_Ability_Statistics/ directory.\")\n",
    "        else:\n",
    "            print(f\"\\nSuccessfully converted {len(files)} files to *_raw_events_with_lineid.csv format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a8f2f",
   "metadata": {},
   "source": [
    "# ÊâπÊ¨°ÂúñÂΩ¢ÁîüÊàêËàáÂÑ≤Â≠ò (Batch Graph Generation and Storage)\n",
    "\n",
    "Ê≠§Ê†∏ÂøÉËôïÁêÜÂñÆÂÖÉÊ†ºË≤†Ë≤¨Â∞áÈ†êËôïÁêÜÁöÑ CSV Ê™îÊ°àËΩâÊèõÁÇ∫ NetworkX ÂúñÂΩ¢ÁµêÊßãÔºå‰∏¶ÂÑ≤Â≠òÁõ∏ÈóúÁöÑ‰∏≠ÁπºË≥áÊñô„ÄÇ\n",
    "\n",
    "## ‰∏ªË¶ÅËôïÁêÜÊ≠•È©ü (Main Processing Steps)\n",
    "\n",
    "### 1. Ê™îÊ°àËø¥ÂúàËôïÁêÜ (File Loop Processing)\n",
    "- ‰ΩøÁî® `tqdm` È°ØÁ§∫Êï¥È´îËôïÁêÜÈÄ≤Â∫¶\n",
    "- ÈÄê‰∏ÄËôïÁêÜÊØèÂÄã `*_raw_events_with_lineid.csv` Ê™îÊ°à\n",
    "- Ëá™ÂãïË∑≥ÈÅéÁÑ°Ê≥ïËß£Á¢ºÁöÑÊ™îÊ°à\n",
    "\n",
    "### 2. CSV Ë≥áÊñôËß£Êûê (CSV Data Parsing)\n",
    "Â∞çÊØèÂÄã CSV Ê™îÊ°àÂü∑Ë°å‰ª•‰∏ãÊìç‰ΩúÔºö\n",
    "- **Â§öÁ∑®Á¢ºÂòóË©¶**: UTF-8, UTF-8-BOM, CP950\n",
    "- **Ê¨Ñ‰ΩçÂ∞çÊáâ**: Â∞á CSV Ê¨Ñ‰ΩçÂ∞çÊáâÂà∞Ê®ôÊ∫ñÂåñÁöÑ‰∫ã‰ª∂ÁµêÊßã\n",
    "- **UUID ÁîüÊàê**: ÁÇ∫ÊØèÂÄãË≥áÊ∫êÁîüÊàêÂîØ‰∏ÄÊ®ôË≠òÁ¨¶\n",
    "\n",
    "### 3. ‰∫ã‰ª∂ÂàÜÈ°ûËàáÁØÄÈªûÂª∫Á´ã (Event Classification and Node Creation)\n",
    "\n",
    "#### Ê™îÊ°àÁ≥ªÁµ±‰∫ã‰ª∂ (File System Events)\n",
    "- Ë≠òÂà•ÈóúÈçµÂ≠ó: 'file', 'file system'\n",
    "- ÁØÄÈªûÂ±¨ÊÄß: UUID, Name, Type=\"File\"\n",
    "- Ë∑ØÂæëËôïÁêÜ: Âæû 'Path' Êàñ 'Content' Ê¨Ñ‰ΩçÊèêÂèñ\n",
    "\n",
    "#### Ë®ªÂÜäË°®‰∫ã‰ª∂ (Registry Events)\n",
    "- Ë≠òÂà•ÈóúÈçµÂ≠ó: 'registry'\n",
    "- ÁØÄÈªûÂ±¨ÊÄß: UUID, Key, Type=\"Registry\"\n",
    "- ÈçµÂÄºËôïÁêÜ: Âæû 'Path' Êàñ 'Detail' Ê¨Ñ‰ΩçÊèêÂèñ\n",
    "\n",
    "#### Á∂≤Ë∑Ø‰∫ã‰ª∂ (Network Events)\n",
    "- Ë≠òÂà•ÈóúÈçµÂ≠ó: 'network'\n",
    "- ÁØÄÈªûÂ±¨ÊÄß: UUID, Dstaddress, Type=\"Network\"\n",
    "- IP ÊèêÂèñ: ‰ΩøÁî®Ê≠£Ë¶èË°®ÈÅîÂºèÊèêÂèñ IP ‰ΩçÂùÄ\n",
    "\n",
    "#### Á®ãÂ∫è‰∫ã‰ª∂ (Process Events)\n",
    "- Ë≠òÂà•ÈóúÈçµÂ≠ó: 'process'\n",
    "- ÁâπÊÆäËôïÁêÜ: ÂçÄÂàÜ Process Create Âíå Process Start\n",
    "- PID Ëß£Êûê: Âæû Detail Ê¨Ñ‰ΩçÊàñ PID Ê¨Ñ‰ΩçÊèêÂèñÁ®ãÂ∫è ID\n",
    "\n",
    "### 4. ÊôÇÈñìÊà≥Ë®òËôïÁêÜ (Timestamp Processing)\n",
    "ÂòóË©¶Â§öÁ®ÆÊôÇÈñìÊ†ºÂºèÔºö\n",
    "- `\"%m/%d/%Y %I:%M:%S %p\"` (ÁæéÂºèÊó•ÊúüÊôÇÈñì)\n",
    "- `\"%Y-%m-%d %H:%M:%S\"` (ISO Ê†ºÂºè)\n",
    "- Â§±ÊïóÊôÇ‰ΩøÁî®Ë°åËôü‰ΩúÁÇ∫Â∫èÂàóËôü\n",
    "\n",
    "### 5. ÂúñÂΩ¢ÁîüÊàêËàáÂÑ≤Â≠ò (Graph Generation and Storage)\n",
    "- Ë™øÁî® `generate_query_graph()` ÂáΩÊï∏Âª∫Á´ã NetworkX ÂúñÂΩ¢\n",
    "- Â∞áÂúñÂΩ¢ÂÑ≤Â≠òÁÇ∫ pickle Ê†ºÂºè (`*.pkl`)\n",
    "- Â∞áÈÇäÁöÑ‰∏≠ÁπºË≥áÊñôÂÑ≤Â≠òÁÇ∫ JSON Ê†ºÂºè (`*_edge_metadata.json`)\n",
    "\n",
    "### 6. ‰∏≠ÁπºË≥áÊñôÁµêÊßã (Metadata Structure)\n",
    "ÈÇäÁöÑ‰∏≠ÁπºË≥áÊñôÂåÖÂê´Ôºö\n",
    "- `line_id`: ÂéüÂßãÊó•Ë™åË°åËôü\n",
    "- `operation`: Êìç‰ΩúÈ°ûÂûã\n",
    "- `timestamp`: ÊôÇÈñìÊà≥Ë®ò\n",
    "- `technique`: ÊäÄË°ìÂàÜÈ°û\n",
    "- `src_process`: ‰æÜÊ∫êÁ®ãÂ∫èË≥áË®ä\n",
    "- `src_pid`: ‰æÜÊ∫êÁ®ãÂ∫è ID\n",
    "- `dst_resource`: ÁõÆÊ®ôË≥áÊ∫ê\n",
    "- `dst_type`: ÁõÆÊ®ôÈ°ûÂûã\n",
    "\n",
    "Ê≠§Ê≠•È©üÁöÑËº∏Âá∫ÁÇ∫ÊØèÂÄãÊó•Ë™åÊ™îÊ°àÁî¢Áîü‰∏ÄÁµÑÂÆåÊï¥ÁöÑÂúñÂΩ¢Ë≥áÊñôÔºåÂåÖÊã¨ÂúñÂΩ¢ÁµêÊßãÂíåË©≥Á¥∞ÁöÑ‰∏≠ÁπºË≥áÊñôÔºåÁÇ∫ÂæåÁ∫åÁöÑÂàÜÊûêÂíåË¶ñË¶∫ÂåñÊèê‰æõÂü∫Á§é„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(files)} file(s) to process\")\n",
    "print(\"Building provenance graphs...\")\n",
    "\n",
    "for idx, f in enumerate(tqdm(files, desc=\"Processing files\")):\n",
    "    # print(f\"\\nüìÅ Processing file {idx+1}/{len(files)}: {os.path.basename(f)}\")\n",
    "    \n",
    "    if True:\n",
    "        # Convert CSV rows to the campaign_events format expected by generate_query_graph\n",
    "        import csv\n",
    "        from datetime import datetime\n",
    "        import re\n",
    "        campaign_events = []\n",
    "        \n",
    "        # Try different encodings to handle various CSV formats\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'cp950']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(f, newline='', encoding=encoding) as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    rows = list(reader)\n",
    "                    break  # If successful, break out of the encoding loop\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try next encoding\n",
    "        else:\n",
    "            # If all encodings fail, skip this file\n",
    "            print(f\"‚ùå Could not decode file {os.path.basename(f)} with any supported encoding\")\n",
    "            continue\n",
    "            \n",
    "        for i, row in enumerate(tqdm(rows, desc=\"Converting CSV\", leave=False)):\n",
    "            src_name = row.get('Process Name') or row.get('Image Path') or row.get('User') or 'unknown'\n",
    "            pid = row.get('PID') or row.get('Parent PID') or ''\n",
    "            try:\n",
    "                pid_int = int(pid)\n",
    "            except:\n",
    "                pid_int = 0\n",
    "            src_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{src_name}_{pid_int}\"))\n",
    "            srcNode = {\"UUID\": src_uuid, \"Name\": src_name, \"Pid\": pid_int, \"Type\": \"Process\"}\n",
    "\n",
    "            event_class = (row.get('Event Class') or '').lower()\n",
    "            operation = row.get('Operation') or row.get('srcEvent') or 'unknown'\n",
    "            relation = operation\n",
    "            dstNode = None\n",
    "            if 'file' in event_class or 'file system' in event_class:\n",
    "                path = row.get('Path') or row.get('Content') or ''\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, path)) if path else src_uuid\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Name\": os.path.basename(path) if path else path, \"Type\": \"File\"}\n",
    "            elif 'registry' in event_class:\n",
    "                key = row.get('Path') or row.get('Detail') or ''\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, key)) if key else src_uuid\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Key\": key, \"Type\": \"Registry\"}\n",
    "            elif 'network' in event_class:\n",
    "                addr = ''\n",
    "                for field in ('Detail','Path','Content','srcEvent'):\n",
    "                    text = row.get(field,'')\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    m = re.search(r'(?:\\b)(?:\\d{1,3}\\.){3}\\d{1,3}(?:\\b)', text)\n",
    "                    if m:\n",
    "                        addr = m.group(0); break\n",
    "                if addr:\n",
    "                    dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, addr))\n",
    "                    dstNode = {\"UUID\": dst_uuid, \"Dstaddress\": addr, \"Type\": \"Network\"}\n",
    "            elif 'process' in event_class:\n",
    "                path = row.get('Path') or row.get('Image Path') or ''\n",
    "                dst_name = os.path.basename(path.replace('\\\\', '/').strip()) if path else ''\n",
    "                dst_name = dst_name.lstrip('/\\\\').strip()\n",
    "                #print(f\"Processing process event: {dst_name} from {path}\")\n",
    "                op_lower = (operation or '').lower()\n",
    "\n",
    "                # Determine destination PID based on operation subtype\n",
    "                dst_pid = 0\n",
    "                if 'create' in op_lower:  # Process Create\n",
    "                    details_text = row.get('Detail') or row.get('Details') or ''\n",
    "                    m = re.search(r'PID\\s*:\\s*(\\d+)', details_text)\n",
    "                    if m:\n",
    "                        try:\n",
    "                            dst_pid = int(m.group(1))\n",
    "                        except:\n",
    "                            dst_pid = 0\n",
    "                    else:\n",
    "                        # Fallback to PID / Parent PID\n",
    "                        dst_pid_val = row.get('PID') or row.get('Parent PID') or 0\n",
    "                        try:\n",
    "                            dst_pid = int(dst_pid_val)\n",
    "                        except:\n",
    "                            dst_pid = 0\n",
    "                else:  # Process Start or others\n",
    "                    dst_pid_val = row.get('PID') or row.get('Parent PID') or 0\n",
    "                    try:\n",
    "                        dst_pid = int(dst_pid_val)\n",
    "                    except:\n",
    "                        dst_pid = 0\n",
    "                #print(f\"  Destination PID: {dst_pid}\")\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{dst_name}_{dst_pid}\"))\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Name\": dst_name, \"Pid\": dst_pid, \"Type\": \"Process\"}\n",
    "\n",
    "            # timestamp parsing: try common formats, otherwise use row index\n",
    "            ts = None\n",
    "            dtstr = row.get('Date & Time') or row.get('Date') or ''\n",
    "            try:\n",
    "                ts = int(datetime.strptime(dtstr, \"%m/%d/%Y %I:%M:%S %p\").timestamp())\n",
    "            except:\n",
    "                try:\n",
    "                    ts = int(datetime.strptime(dtstr, \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "                except:\n",
    "                    ts = i\n",
    "\n",
    "            # include line id if present in CSV\n",
    "            line_id = row.get('lineid') or row.get('LineID') or row.get('line_id') or ''\n",
    "\n",
    "            ev = {\"srcNode\": srcNode, \"dstNode\": dstNode, \"relation\": relation, \"timestamp\": ts, \"line_id\": str(line_id) if line_id != '' else None}\n",
    "            ev[\"label\"] = f\"CSV_{operation}\"\n",
    "            campaign_events.append(ev)\n",
    "\n",
    "    # print(f\"  ‚úÖ Loaded {len(campaign_events)} events\")\n",
    "\n",
    "    eventname = os.path.basename(f).replace('_raw_events_with_lineid.csv', '')\n",
    "    save_path = os.path.join(GRAPH_PATH, eventname)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    G, edge_metadata = generate_query_graph(campaign_events)\n",
    "    \n",
    "\n",
    "    # Save graph and metadata\n",
    "    with open(f\"{save_path}/{eventname}.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(G, fp)\n",
    "    with open(f\"{save_path}/{eventname}_edge_metadata.json\", \"w\") as fp:\n",
    "        # Convert edge metadata to JSON-serializable format\n",
    "        json_metadata = {}\n",
    "        for (src, dst, key), data in edge_metadata.items():\n",
    "            edge_id = f\"{src}‚Üí{dst}#{key}\"\n",
    "            json_metadata[edge_id] = {\n",
    "                'line_id': data['line_id'],\n",
    "                'operation': data['operation'],\n",
    "                'timestamp': data['timestamp'],\n",
    "                'technique': data['technique'],\n",
    "                'src_process': data['src_process'],\n",
    "                'src_pid': data['src_pid'],\n",
    "                'dst_resource': data['dst_resource'],\n",
    "                'dst_type': data['dst_type']\n",
    "            }\n",
    "        json.dump(json_metadata, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dae60",
   "metadata": {},
   "source": [
    "# ÂúñÂΩ¢Ê™îÊ°àËíêÈõÜËàáÈ©óË≠â (Graph File Collection and Validation)\n",
    "\n",
    "Ê≠§ÂñÆÂÖÉÊ†ºË≤†Ë≤¨ËíêÈõÜÊâÄÊúâÂ∑≤ÁîüÊàêÁöÑÂúñÂΩ¢Ê™îÊ°àÔºå‰∏¶È©óË≠âÂÖ∂ÂÆåÊï¥ÊÄßÔºåÁÇ∫ÂæåÁ∫åÁöÑË¶ñË¶∫ÂåñËôïÁêÜÂÅöÊ∫ñÂÇô„ÄÇ\n",
    "\n",
    "## ÂäüËÉΩË™™Êòé (Functionality Description)\n",
    "\n",
    "### 1. ÁõÆÈåÑÁµêÊßãÊéÉÊèè (Directory Structure Scanning)\n",
    "- ÊéÉÊèè `GRAPH_PATH` (È†êË®≠ÁÇ∫ \"Graphs\") ÁõÆÈåÑ‰∏ãÁöÑÊâÄÊúâÂ≠êÁõÆÈåÑ\n",
    "- ÊØèÂÄãÂ≠êÁõÆÈåÑÂ∞çÊáâ‰∏ÄÂÄãËôïÁêÜÈÅéÁöÑÊó•Ë™åÊ™îÊ°à\n",
    "- ÁõÆÈåÑÂêçÁ®±ÈÄöÂ∏∏ÁÇ∫ÂéüÂßãÊ™îÊ°àÁöÑÂü∫Á§éÂêçÁ®± (‰∏çÂê´ÂâØÊ™îÂêç)\n",
    "\n",
    "### 2. Pickle Ê™îÊ°àÈ©óË≠â (Pickle File Validation)\n",
    "- Âú®ÊØèÂÄãÂ≠êÁõÆÈåÑ‰∏≠Â∞ãÊâæ `{eventname}.pkl` Ê™îÊ°à\n",
    "- ÈÄô‰∫õ pickle Ê™îÊ°àÂåÖÂê´ÂÆåÊï¥ÁöÑ NetworkX ÂúñÂΩ¢Áâ©‰ª∂\n",
    "- Âè™ÊúâÂ≠òÂú®Â∞çÊáâ pickle Ê™îÊ°àÁöÑÁõÆÈåÑÊâçÊúÉË¢´Âä†ÂÖ•ËôïÁêÜÊ∏ÖÂñÆ\n",
    "\n",
    "### 3. ÂèØÁî®ÂúñÂΩ¢ÁôªÈåÑ (Available Graphs Registration)\n",
    "- Â∞áÊâæÂà∞ÁöÑÊúâÊïàÂúñÂΩ¢Ê™îÊ°àË∑ØÂæëÂä†ÂÖ• `available_graphs` Ê∏ÖÂñÆ\n",
    "- Êèê‰æõÂç≥ÊôÇÁöÑÁôºÁèæÂõûÈ•ãÔºåÈ°ØÁ§∫ÊâæÂà∞ÁöÑÂúñÂΩ¢Ê™îÊ°à\n",
    "- ÁÇ∫ÂæåÁ∫åÁöÑÊâπÊ¨°Ë¶ñË¶∫ÂåñËôïÁêÜÂª∫Á´ãÊ™îÊ°àÊ∏ÖÂñÆ\n",
    "\n",
    "### 4. ÂìÅË≥™‰øùË≠â (Quality Assurance)\n",
    "Ê≠§Ê≠•È©üÁ¢∫‰øùÔºö\n",
    "- ÊâÄÊúâÂúñÂΩ¢Ê™îÊ°àÈÉΩÊòØÂÆåÊï¥‰∏îÂèØËÆÄÂèñÁöÑ\n",
    "- ÈÅøÂÖçËôïÁêÜ‰∏çÂÆåÊï¥ÊàñÊêçÂ£ûÁöÑË≥áÊñô\n",
    "- Êèê‰æõÊ∏ÖÊ•öÁöÑËôïÁêÜÁØÑÂúçË≥áË®ä\n",
    "\n",
    "ÈÄôÂÄãÈ©óË≠âÊ≠•È©üÊòØÊâπÊ¨°Ë¶ñË¶∫ÂåñËôïÁêÜÁöÑÈáçË¶ÅÂâçÁΩÆ‰ΩúÊ•≠ÔºåÁ¢∫‰øùÂæåÁ∫åËôïÁêÜÁöÑÂèØÈù†ÊÄßÂíåÊïàÁéá„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed452c5",
   "metadata": {},
   "source": [
    "# Gather all graphs for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_base_dir = GRAPH_PATH  \n",
    "available_graphs = []\n",
    "\n",
    "# Look for .pkl files in subdirectories of Graphs/\n",
    "for subdir in os.listdir(graph_base_dir):\n",
    "    subdir_path = os.path.join(graph_base_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Look for {eventname}.pkl in each subdirectory\n",
    "        pkl_file = os.path.join(subdir_path, f\"{subdir}.pkl\")\n",
    "        if os.path.exists(pkl_file):\n",
    "            available_graphs.append(pkl_file)\n",
    "            print(f\"\\rFound graph: {pkl_file}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28303cec",
   "metadata": {},
   "source": [
    "# ‰∫íÂãïÂºèË¶ñË¶∫ÂåñÁîüÊàê (Interactive Visualization Generation)\n",
    "\n",
    "Ê≠§ÂñÆÂÖÉÊ†ºÁÇ∫ÊâÄÊúâÂèØÁî®ÁöÑÂúñÂΩ¢ÁîüÊàê‰∫íÂãïÂºèÁöÑ HTML Ë¶ñË¶∫Âåñ‰ªãÈù¢„ÄÇÁõÆÂâçÂ∞àÊ≥®ÊñºÂü∫ÊñºÊ¢ùÁõÆÁØÑÂúçÁöÑ‰∫íÂãïÂºèË¶ñË¶∫Âåñ„ÄÇ\n",
    "\n",
    "## ËôïÁêÜÊµÅÁ®ã (Processing Workflow)\n",
    "\n",
    "### 1. ÊâπÊ¨°ËôïÁêÜË®≠ÁΩÆ (Batch Processing Setup)\n",
    "- ËôïÁêÜÊâÄÊúâÂú®‰∏ä‰∏ÄÊ≠•È©ü‰∏≠ÊâæÂà∞ÁöÑÊúâÊïàÂúñÂΩ¢Ê™îÊ°à\n",
    "- ÂàùÂßãÂåñÊàêÂäüÂíåÂ§±ÊïóÁöÑË¶ñË¶∫ÂåñË®àÊï∏Âô®\n",
    "- Êèê‰æõË©≥Á¥∞ÁöÑËôïÁêÜÈÄ≤Â∫¶Â†±Âëä\n",
    "\n",
    "### 2. ÂøÖË¶ÅÊ™îÊ°àÊ™¢Êü• (Required Files Validation)\n",
    "Â∞çÊØèÂÄãÂúñÂΩ¢Ê™îÊ°àÔºåÁ≥ªÁµ±È©óË≠â‰ª•‰∏ãÊ™îÊ°àÁöÑÂ≠òÂú®Ôºö\n",
    "- **ÂúñÂΩ¢Ê™îÊ°à**: `{eventname}.pkl` (NetworkX ÂúñÂΩ¢Áâ©‰ª∂)\n",
    "- **‰∏≠ÁπºË≥áÊñôÊ™îÊ°à**: `{eventname}_edge_metadata.json` (ÈÇäÁöÑË©≥Á¥∞Ë≥áË®ä)\n",
    "- **È†êÊ∏¨Ê™îÊ°à**: `Caldera_Ability_Predictions/{eventname}.csv` (ÂèØÈÅ∏ÁöÑÊÉ°ÊÑèË°åÁÇ∫È†êÊ∏¨)\n",
    "\n",
    "### 3. ÊôÇÈñìÁØÑÂúçÂàÜÊûê (Time Range Analysis)\n",
    "- ‰ΩøÁî® `get_graph_time_range()` ÂáΩÊï∏ÂàÜÊûêÂúñÂΩ¢ÁöÑÊôÇÈñìË∑®Â∫¶\n",
    "- Ë®àÁÆóÊúÄÂ∞èÂíåÊúÄÂ§ßÊôÇÈñìÊà≥Ë®ò\n",
    "- Ë∑≥ÈÅéÊ≤íÊúâÊôÇÈñìË≥áË®äÁöÑÂúñÂΩ¢\n",
    "\n",
    "### 4. ÊÉ°ÊÑèË°åÁÇ∫È†êÊ∏¨Êï¥Âêà (Malicious Behavior Prediction Integration)\n",
    "ÊîØÊè¥Â§öÁ®ÆÈ†êÊ∏¨Ê™îÊ°àÊ†ºÂºèÔºö\n",
    "\n",
    "#### Ê†ºÂºè A: LineID + Type + Score\n",
    "- Ê¨Ñ‰Ωç: `LineID`, `Type`, `Score`\n",
    "- Ê¢ù‰ª∂: `Score >= 1.0`\n",
    "- È°ûÂûã: `src`, `dst`, Êàñ `both`\n",
    "\n",
    "#### Ê†ºÂºè B: line_id + prediction/label\n",
    "- Ê¨Ñ‰Ωç: `line_id`, `prediction`/`is_malicious`/`label`\n",
    "- Ê¢ù‰ª∂: È†êÊ∏¨ÂÄºÁÇ∫ 1 ÊàñÊ®ôÁ±§ÁÇ∫ 'malicious'\n",
    "\n",
    "### 5. Ë¶ñË¶∫ÂåñÁîüÊàê (Visualization Generation)\n",
    "- Ë™øÁî® `create_interactive_entry_visualization()` ÁîüÊàê‰∫íÂãïÂºè HTML\n",
    "- ÊîØÊè¥Âü∫ÊñºÊ¢ùÁõÆÂ∫èËôüÁöÑÁØ©ÈÅ∏ÂäüËÉΩ\n",
    "- Êï¥ÂêàÊÉ°ÊÑèË°åÁÇ∫Ê®ôË®ò (Â¶ÇÊûúÊúâÈ†êÊ∏¨Ë≥áÊñô)\n",
    "\n",
    "### 6. Áµ±Ë®àË≥áË®äÊî∂ÈõÜ (Statistics Collection)\n",
    "ÁÇ∫ÊØèÂÄãÊàêÂäüÁöÑË¶ñË¶∫ÂåñÊî∂ÈõÜÔºö\n",
    "- **ÁØÄÈªûÊï∏Èáè**: ÂúñÂΩ¢‰∏≠ÁöÑÁØÄÈªûÁ∏ΩÊï∏\n",
    "- **ÈÇäÊï∏Èáè**: ÂúñÂΩ¢‰∏≠ÁöÑÈÇäÁ∏ΩÊï∏\n",
    "- **ÊôÇÈñìË∑®Â∫¶**: ‰∫ã‰ª∂ÁöÑÊôÇÈñìÁØÑÂúç\n",
    "- **ÊÉ°ÊÑèÊ®ôË®òÊï∏Èáè**: REAPr ÊàñÂÖ∂‰ªñÈ†êÊ∏¨Á≥ªÁµ±Ê®ôË®òÁöÑÊÉ°ÊÑèË°åÁÇ∫Êï∏Èáè\n",
    "- **È†êÊ∏¨ÂèØÁî®ÊÄß**: ÊòØÂê¶ÊúâÂèØÁî®ÁöÑÈ†êÊ∏¨Ë≥áÊñô\n",
    "\n",
    "### 7. ÁµêÊûúÂ†±Âëä (Results Reporting)\n",
    "ÁîüÊàêË©≥Á¥∞ÁöÑËôïÁêÜÂ†±ÂëäÔºåÂåÖÊã¨Ôºö\n",
    "- ÊàêÂäüËôïÁêÜÁöÑË¶ñË¶∫ÂåñÊï∏ÈáèÂíåË©≥Á¥∞Ë≥áË®ä\n",
    "- Â§±ÊïóÁöÑË¶ñË¶∫ÂåñÂèäÂÖ∂ÂéüÂõ†\n",
    "- ÊØèÂÄãÂúñÂΩ¢ÁöÑÁµ±Ë®àÊëòË¶ÅË°®Ê†º\n",
    "\n",
    "## Ê≥®ÊÑè‰∫ãÈ†Ö (Notes)\n",
    "- **ÊôÇÈñìË¶ñË¶∫Âåñ**: ÁõÆÂâçÂ∑≤ÂÅúÁî®ÊôÇÈñìÁØÑÂúçÁØ©ÈÅ∏ÂäüËÉΩÔºåÂõ†ÁÇ∫Âú®Â§ßÂ§öÊï∏ÊÉÖÊ≥Å‰∏ãÁî®ËôïÊúâÈôê\n",
    "- **Ê¢ùÁõÆË¶ñË¶∫Âåñ**: Â∞àÊ≥®ÊñºÂü∫ÊñºÊó•Ë™åÊ¢ùÁõÆÈ†ÜÂ∫èÁöÑ‰∫íÂãïÂºèÂàÜÊûê\n",
    "- **ÈåØË™§ËôïÁêÜ**: Ëá™ÂãïË∑≥ÈÅéÊúâÂïèÈ°åÁöÑÊ™îÊ°à‰∏¶Ë®òÈåÑÈåØË™§ÂéüÂõ†\n",
    "\n",
    "Ê≠§ÂäüËÉΩÁÇ∫ÂàÜÊûêÂ∏´Êèê‰æõ‰∫ÜÂº∑Â§ßÁöÑ‰∫íÂãïÂºèÂ∑•ÂÖ∑ÔºåÂèØ‰ª•Ê∑±ÂÖ•Êé¢Á¥¢ÊîªÊìäÂ∫èÂàóÂíåÁ≥ªÁµ±Ë°åÁÇ∫Ê®°Âºè„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54101dcf",
   "metadata": {},
   "source": [
    "# Generate visualization that can be filtered by time and entry range\n",
    "\n",
    "Time range is disabled for now since its almost useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Interactive Time Visualizations for ALL Available Graphs\n",
    "\n",
    "print(f\"üîç Creating interactive time visualizations for {len(available_graphs)} graphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_visualizations = []\n",
    "failed_visualizations = []\n",
    "\n",
    "for graph_file in available_graphs:\n",
    "    graph_basename = os.path.basename(os.path.dirname(graph_file))\n",
    "    graph_dir = os.path.dirname(graph_file)\n",
    "    edge_metadata_file = os.path.join(graph_dir, f\"{graph_basename}_edge_metadata.json\")\n",
    "    predictions_file = f\"Caldera_Ability_Predictions/{graph_basename}.csv\"\n",
    "    \n",
    "    print(f\"\\nüìä Processing: {graph_basename}\")\n",
    "    \n",
    "    # Check if required files exist\n",
    "    if not os.path.exists(edge_metadata_file):\n",
    "        print(f\"‚ùå Missing edge metadata file - skipping\")\n",
    "        failed_visualizations.append((graph_basename, \"Missing metadata\"))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load the graph and metadata\n",
    "        G = pickle.load(open(graph_file, \"rb\"))\n",
    "        with open(edge_metadata_file, \"r\") as fp:\n",
    "            edge_metadata_json = json.load(fp)\n",
    "        \n",
    "        # Convert edge metadata\n",
    "        edge_metadata = {}\n",
    "        for edge_id, data in edge_metadata_json.items():\n",
    "            parts = edge_id.split('‚Üí')\n",
    "            if len(parts) == 2:\n",
    "                src = parts[0]\n",
    "                dst_key = parts[1].split('#')\n",
    "                if len(dst_key) == 2:\n",
    "                    dst = dst_key[0]\n",
    "                    key = int(dst_key[1]) if dst_key[1].isdigit() else dst_key[1]\n",
    "                    edge_metadata[(src, dst, key)] = data\n",
    "        \n",
    "        # Check if graph has timestamps\n",
    "        min_ts, max_ts = get_graph_time_range(G, edge_metadata)\n",
    "        if min_ts is None:\n",
    "            print(f\"   ‚ö†Ô∏è  No timestamps found - skipping time visualization\")\n",
    "            failed_visualizations.append((graph_basename, \"No timestamps\"))\n",
    "            continue\n",
    "\n",
    "        total_edges = G.number_of_edges()\n",
    "        if total_edges == 0:\n",
    "            print(f\"   ‚ö†Ô∏è  No edges found - skipping entry visualization\")\n",
    "            failed_entry_visualizations.append((graph_basename, \"No edges\"))\n",
    "            continue\n",
    "        \n",
    "        # Load predictions if available\n",
    "        MALICIOUS_SPECS = []\n",
    "        has_predictions = False\n",
    "        \n",
    "        if os.path.exists(predictions_file):\n",
    "            predictions_df = pd.read_csv(predictions_file)\n",
    "            has_predictions = True\n",
    "            \n",
    "            # Handle different CSV formats\n",
    "            if 'LineID' in predictions_df.columns and 'Type' in predictions_df.columns and 'Score' in predictions_df.columns:\n",
    "                malicious_rows = predictions_df[predictions_df['Score'] >= 1.0]\n",
    "                for _, row in malicious_rows.iterrows():\n",
    "                    line_id = str(row['LineID'])\n",
    "                    prediction_type = row['Type'].lower()\n",
    "                    if prediction_type in ['src', 'dst']:\n",
    "                        MALICIOUS_SPECS.append((line_id, prediction_type))\n",
    "                    else:\n",
    "                        MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "            elif 'line_id' in predictions_df.columns:\n",
    "                malicious_rows = predictions_df[\n",
    "                    (predictions_df.get('prediction', 0) == 1) |\n",
    "                    (predictions_df.get('is_malicious', False) == True) |\n",
    "                    (predictions_df.get('label', 'benign').str.lower() == 'malicious')\n",
    "                ]\n",
    "                for _, row in malicious_rows.iterrows():\n",
    "                    line_id = str(row['line_id'])\n",
    "                    MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "        \n",
    "        print(f\"   Entry Count: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        print(f\"   Time range: {datetime.fromtimestamp(min_ts).strftime('%H:%M:%S')} to {datetime.fromtimestamp(max_ts).strftime('%H:%M:%S')}\")\n",
    "        print(f\"   Provided Malicious specs: {len(MALICIOUS_SPECS)}\")\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        output_time_path = os.path.join(graph_dir, f\"interactive_time_{graph_basename}.html\")\n",
    "        output_entry_path = os.path.join(graph_dir, f\"interactive_entry_{graph_basename}.html\")\n",
    "        \n",
    "        # interactive_path = create_interactive_time_visualization(\n",
    "        #     G, edge_metadata, MALICIOUS_SPECS if MALICIOUS_SPECS else None, \n",
    "        #     output_path=output_path\n",
    "        # )\n",
    "        interactive_path = create_interactive_entry_visualization(\n",
    "            G, edge_metadata, MALICIOUS_SPECS if MALICIOUS_SPECS else None, \n",
    "            output_path=output_entry_path\n",
    "        )\n",
    "\n",
    "\n",
    "        # Calculate some time statistics\n",
    "        total_duration = max_ts - min_ts\n",
    "        seconds = total_duration\n",
    "        \n",
    "        successful_visualizations.append({\n",
    "            'graph': graph_basename,\n",
    "            'path': interactive_path,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'duration_seconds': seconds,\n",
    "            'malicious_specs': len(MALICIOUS_SPECS),\n",
    "            'has_predictions': has_predictions\n",
    "        })\n",
    "        \n",
    "        print(f\"   ‚úÖ Interactive visualization created: {os.path.basename(interactive_path)}\")\n",
    "        print(f\"   üìä Duration: {seconds:.1f} seconds, REAPr: {'Yes' if MALICIOUS_SPECS else 'No'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ‚ùå Error: {e}\")\n",
    "        failed_visualizations.append((graph_basename, str(e)))\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessful: {len(successful_visualizations)} | Failed: {len(failed_visualizations)}\")\n",
    "\n",
    "if successful_visualizations:\n",
    "    print(f\"\\nSUCCESSFUL VISUALIZATIONS:\")\n",
    "    print(f\"{'Graph':<35} {'Nodes':<8} {'Edges':<8} {'Duration':<12} {'REAPr':<8} {'File'}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for viz in successful_visualizations:\n",
    "        duration_str = f\"{viz['duration_seconds']:.1f}s\"\n",
    "        reapr_str = \"Yes\" if viz['malicious_specs'] > 0 else \"No\"\n",
    "        filename = os.path.basename(viz['path'])\n",
    "        print(f\"{viz['graph']:<35} {viz['nodes']:<8} {viz['edges']:<8} {duration_str:<12} {reapr_str:<8} {filename}\")\n",
    "    \n",
    "    \n",
    "if failed_visualizations:\n",
    "    print(f\"\\n‚ùå FAILED VISUALIZATIONS:\")\n",
    "    for graph, reason in failed_visualizations:\n",
    "        print(f\"   {graph}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d991198",
   "metadata": {},
   "source": [
    "# ÊîªÊìäÂ∫èÂàóÊ®°ÂºèË¶ñË¶∫Âåñ (Attack Sequence Pattern Visualization)\n",
    "\n",
    "Ê≠§ÂñÆÂÖÉÊ†ºÂ∞àÈñÄÈáùÂ∞çÁâπÂÆöÊ™îÊ°àÈÄ≤Ë°åÊ∑±Â∫¶ÁöÑÊîªÊìäÂ∫èÂàóÊ®°ÂºèÂàÜÊûêÂíåË¶ñË¶∫ÂåñÔºåÂ±ïÁ§∫Á≥ªÁµ±Â¶Ç‰ΩïË≠òÂà•ÂíåÊ®ôË®òÈ†êÂÆöÁæ©ÁöÑÊîªÊìäË°åÁÇ∫Ê®°Âºè„ÄÇ\n",
    "\n",
    "## Ê†∏ÂøÉÂäüËÉΩ (Core Functions)\n",
    "\n",
    "### 1. ÁõÆÊ®ôÊ™îÊ°àË®≠ÂÆö (Target File Configuration)\n",
    "- **ÁõÆÊ®ôÊ™îÊ°à**: `cfb61005899996469ae3023796792ca5` (ÂèØË™øÊï¥)\n",
    "- ËºâÂÖ•Â∞çÊáâÁöÑÂúñÂΩ¢Ê™îÊ°à (`.pkl`) Âíå‰∏≠ÁπºË≥áÊñô (`.json`)\n",
    "- Êï¥ÂêàÈ†êÊ∏¨Ê™îÊ°à‰∏≠ÁöÑÊÉ°ÊÑèË°åÁÇ∫Ê®ôË®ò\n",
    "\n",
    "### 2. Â∫èÂàóÁæ§ÁµÑÂÅµÊ∏¨ (Sequence Group Detection)\n",
    "Ë™øÁî® `find_sequence_groups()` ÂáΩÊï∏Âü∑Ë°åÊô∫ÊÖßÂûãÊ®°ÂºèÊØîÂ∞çÔºö\n",
    "\n",
    "#### ÂèÉÊï∏Ë®≠ÂÆö (Parameter Configuration)\n",
    "- **ÂúñÂΩ¢Áâ©‰ª∂ (G)**: NetworkX ÂúñÂΩ¢ÁµêÊßã\n",
    "- **ÈÇä‰∏≠ÁπºË≥áÊñô (edge_metadata)**: ÂåÖÂê´Êìç‰ΩúË©≥Á¥∞Ë≥áË®ä\n",
    "- **ÊîªÊìäÊ®°Âºè (ATTACK_SEQUENCE_PATTERNS)**: È†êÂÆöÁæ©ÁöÑÊîªÊìäÂ∫èÂàó\n",
    "- **ÊúÄÂ§ßÈñìÈöî (max_gap)**: Â∫èÂàó‰∏≠ÂÖÅË®±ÁöÑÊúÄÂ§ßÊìç‰ΩúÈñìÈöî\n",
    "- **ÁõÆÊ®ôÊ™îÊ°à (target_file)**: Áî®ÊñºÂÅµÈåØÂíåÊó•Ë™åË®òÈåÑ\n",
    "\n",
    "#### ÂÅµÊ∏¨ÊºîÁÆóÊ≥ï (Detection Algorithm)\n",
    "- Â∞ãÊâæÁ¨¶ÂêàÈ†êÂÆöÁæ©Ê®°ÂºèÁöÑÊìç‰ΩúÂ∫èÂàó\n",
    "- Ë®àÁÆóÊ®°ÂºèÂåπÈÖçÁöÑÁΩÆ‰ø°Â∫¶ÂàÜÊï∏\n",
    "- Ë©ï‰º∞Â∫èÂàóÁöÑÂÆåÊï¥ÊÄßÂíåÈ†ÜÂ∫èÊÄß\n",
    "- ÁÇ∫ÊØèÂÄãÊâæÂà∞ÁöÑÁæ§ÁµÑÂàÜÈÖçÂîØ‰∏ÄÁöÑÈ°èËâ≤Ê®ôË®ò\n",
    "\n",
    "### 3. ÂÅµÊ∏¨ÁµêÊûúÂàÜÊûê (Detection Results Analysis)\n",
    "Á≥ªÁµ±Êèê‰æõË©≥Á¥∞ÁöÑÂÅµÊ∏¨Â†±ÂëäÔºö\n",
    "\n",
    "#### Áæ§ÁµÑË≥áË®ä (Group Information)\n",
    "- **Group ID**: Áæ§ÁµÑÂîØ‰∏ÄË≠òÂà•Á¢º\n",
    "- **Pattern Name**: ÂåπÈÖçÁöÑÊîªÊìäÊ®°ÂºèÂêçÁ®±\n",
    "- **Description**: Ê®°ÂºèÁöÑË©≥Á¥∞ÊèèËø∞\n",
    "- **Confidence Score**: ÂåπÈÖçÁΩÆ‰ø°Â∫¶ (0.0-1.0)\n",
    "- **Edge Count**: Áæ§ÁµÑÂåÖÂê´ÁöÑÈÇäÊï∏Èáè\n",
    "- **Matched Operations**: ÂØ¶ÈöõÂåπÈÖçÁöÑÊìç‰ΩúÂ∫èÂàó\n",
    "- **Expected Pattern**: È†êÊúüÁöÑÊìç‰ΩúÊ®°Âºè\n",
    "- **Completeness**: Ê®°ÂºèÂÆåÊï¥ÊÄßË©ïÂàÜ\n",
    "\n",
    "### 4. Â∫èÂàóÁæ§ÁµÑË¶ñË¶∫Âåñ (Sequence Group Visualization)\n",
    "Ë™øÁî® `create_sequence_grouped_visualization()` ÁîüÊàêÂ∞àÈñÄÁöÑË¶ñË¶∫ÂåñÔºö\n",
    "\n",
    "#### Ë¶ñË¶∫ÂåñÁâπËâ≤ (Visualization Features)\n",
    "- **È°èËâ≤Á∑®Á¢º**: ÊØèÁ®ÆÊîªÊìäÊ®°Âºè‰ΩøÁî®ÁâπÂÆöÈ°èËâ≤\n",
    "- **Áæ§ÁµÑÈ´ò‰∫Æ**: Â±¨ÊñºÂêå‰∏ÄÊîªÊìäÂ∫èÂàóÁöÑÈÇä‰ΩøÁî®Áõ∏ÂêåÈ°èËâ≤\n",
    "- **Êú™ÂàÜÁµÑÈÇä**: ‰∏çÂ±¨Êñº‰ªª‰ΩïÊ®°ÂºèÁöÑÈÇä‰ª•ÁÅ∞Ëâ≤È°ØÁ§∫\n",
    "- **‰∫íÂãïÂºè‰ªãÈù¢**: ÊîØÊè¥ÊªëÈº†Êá∏ÂÅúÊü•ÁúãË©≥Á¥∞Ë≥áË®ä\n",
    "\n",
    "### 5. Ë¶ÜËìãÁéáÂàÜÊûê (Coverage Analysis)\n",
    "Á≥ªÁµ±Ë®àÁÆó‰∏¶Â†±ÂëäÊ®°ÂºèË¶ÜËìãÁµ±Ë®àÔºö\n",
    "\n",
    "#### Áµ±Ë®àÊåáÊ®ô (Statistical Metrics)\n",
    "- **Áæ§ÁµÑÂåñÈÇäÊï∏**: Ë¢´Ê≠∏È°ûÂà∞ÊîªÊìäÊ®°ÂºèÁöÑÈÇäÊï∏Èáè\n",
    "- **Á∏ΩÈÇäÊï∏**: ÂúñÂΩ¢‰∏≠ÁöÑÈÇäÁ∏ΩÊï∏\n",
    "- **Ë¶ÜËìãÁéáÁôæÂàÜÊØî**: Áæ§ÁµÑÂåñÈÇäÂç†Á∏ΩÈÇäÊï∏ÁöÑÊØî‰æã\n",
    "- **Êú™Áæ§ÁµÑÂåñÈÇäÊï∏**: Êú™ÂåπÈÖç‰ªª‰ΩïÊ®°ÂºèÁöÑÈÇäÊï∏Èáè\n",
    "\n",
    "#### Ê®°ÂºèÂàÜ‰ΩàÂàÜÊûê (Pattern Distribution Analysis)\n",
    "- ÊØèÁ®ÆÊîªÊìäÊ®°ÂºèÂåπÈÖçÁöÑÈÇäÊï∏Èáè\n",
    "- ÂêÑÊ®°ÂºèÂú®Êï¥È´îÂúñÂΩ¢‰∏≠ÁöÑÂç†ÊØî\n",
    "- ÊåâÈÇäÊï∏ÈáèÊéíÂ∫èÁöÑÊ®°ÂºèÈáçË¶ÅÊÄßÊéíÂêç\n",
    "\n",
    "### 6. ÈåØË™§ËôïÁêÜËàáÂÅµÈåØ (Error Handling and Debugging)\n",
    "- ÂÆåÊï¥ÁöÑ‰æãÂ§ñËôïÁêÜÂíåÈåØË™§Â†±Âëä\n",
    "- Ë©≥Á¥∞ÁöÑÂ†ÜÁñäËøΩËπ§Ë≥áË®ä\n",
    "- Ê™îÊ°àÂ≠òÂú®ÊÄßÈ©óË≠â\n",
    "\n",
    "## ÊáâÁî®ÂÉπÂÄº (Application Value)\n",
    "Ê≠§ÂäüËÉΩÂ∞çÁ∂≤Ë∑ØÂÆâÂÖ®ÂàÜÊûêÁâπÂà•ÈáçË¶ÅÔºö\n",
    "- **ÊîªÊìäË∑ØÂæëË¶ñË¶∫Âåñ**: Ê∏ÖÊ•öÂ±ïÁ§∫ÊîªÊìäËÄÖÁöÑË°åÁÇ∫Â∫èÂàó\n",
    "- **Ê®°ÂºèË≠òÂà•**: Ëá™ÂãïË≠òÂà•Â∑≤Áü•ÁöÑÊîªÊìäÊäÄË°ì\n",
    "- **Áï∞Â∏∏ÂÅµÊ∏¨**: Á™ÅÂá∫È°ØÁ§∫ÂèØÁñëÁöÑË°åÁÇ∫Ê®°Âºè\n",
    "- **‰∫ã‰ª∂ÈóúËÅØ**: Â∞áÁõ∏ÈóúÁöÑÁ≥ªÁµ±‰∫ã‰ª∂Áæ§ÁµÑÂåñ\n",
    "\n",
    "ÈÄôÁ®ÆÂü∫ÊñºÊ®°ÂºèÁöÑÂàÜÊûêÊñπÊ≥ïÊúâÂä©ÊñºÂÆâÂÖ®ÂàÜÊûêÂ∏´Âø´ÈÄüÁêÜËß£Ë§áÈõúÁöÑÊîªÊìäÂ†¥ÊôØÔºå‰∏¶Ë≠òÂà•ÊΩõÂú®ÁöÑÂ®ÅËÑÖË°åÁÇ∫„ÄÇ"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc582a1",
   "metadata": {},
   "source": [
    "# Generate visualization that colors the found patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c789935",
   "metadata": {},
   "source": [
    "# ÂñÆ‰∏ÄÊ™îÊ°àÊ∑±Â∫¶Â∫èÂàóÂàÜÊûê (Single File Deep Sequence Analysis)\n",
    "\n",
    "Ê≠§ÊúÄÁµÇÂñÆÂÖÉÊ†ºÂ±ïÁ§∫Â¶Ç‰ΩïÂ∞çÁâπÂÆöÁöÑÊó•Ë™åÊ™îÊ°àÂü∑Ë°åÊ∑±Â∫¶ÁöÑÊîªÊìäÂ∫èÂàóÂàÜÊûêÔºåÁµêÂêà‰∫ÜÂúñÂΩ¢ËºâÂÖ•„ÄÅÊ®°ÂºèÂÅµÊ∏¨„ÄÅÈ†êÊ∏¨Êï¥ÂêàÂíåÂ∞àÈñÄÁöÑÂ∫èÂàóË¶ñË¶∫Âåñ„ÄÇ\n",
    "\n",
    "## ÂàÜÊûêÊµÅÁ®ã (Analysis Workflow)\n",
    "\n",
    "### 1. Ê™îÊ°àÂàùÂßãÂåñ (File Initialization)\n",
    "- **ÁõÆÊ®ôÊ™îÊ°à**: `cfb61005899996469ae3023796792ca5` (ÁØÑ‰æãÊ™îÊ°à)\n",
    "- ÊßãÂª∫ÂÆåÊï¥ÁöÑÊ™îÊ°àË∑ØÂæëÁµêÊßã\n",
    "- È©óË≠âÊâÄÊúâÂøÖË¶ÅÊ™îÊ°àÁöÑÂ≠òÂú®ÊÄß\n",
    "\n",
    "### 2. Ë≥áÊñôËºâÂÖ•ËàáÊï¥Âêà (Data Loading and Integration)\n",
    "\n",
    "#### ÂúñÂΩ¢Ë≥áÊñôËºâÂÖ• (Graph Data Loading)\n",
    "- ËºâÂÖ• pickle Ê†ºÂºèÁöÑ NetworkX ÂúñÂΩ¢Áâ©‰ª∂\n",
    "- Ëß£Êûê JSON Ê†ºÂºèÁöÑÈÇä‰∏≠ÁπºË≥áÊñô\n",
    "- ÈáçÂª∫ÈÇäË≠òÂà•Á¢ºÂà∞‰∏≠ÁπºË≥áÊñôÁöÑÂ∞çÊáâÈóú‰øÇ\n",
    "\n",
    "#### ÈÇä‰∏≠ÁπºË≥áÊñôËΩâÊèõ (Edge Metadata Conversion)\n",
    "Â∞á JSON ‰∏≠ÁöÑÂ≠ó‰∏≤ÂΩ¢ÂºèÈÇäË≠òÂà•Á¢ºËΩâÊèõÂõûÂÖÉÁµÑÊ†ºÂºèÔºö\n",
    "- Ëß£Êûê `src‚Üídst#key` Ê†ºÂºèÁöÑÈÇäË≠òÂà•Á¢º\n",
    "- ÈáçÂª∫ `(src, dst, key)` ÂÖÉÁµÑ‰ΩúÁÇ∫Â≠óÂÖ∏ÈçµÂÄº\n",
    "- Á¢∫‰øùË≥áÊñôÁµêÊßãËàáÂúñÂΩ¢Áâ©‰ª∂ÁöÑ‰∏ÄËá¥ÊÄß\n",
    "\n",
    "#### È†êÊ∏¨Ë≥áÊñôÊï¥Âêà (Prediction Data Integration)\n",
    "ÊîØÊè¥Â§öÁ®ÆÈ†êÊ∏¨Ê™îÊ°àÊ†ºÂºèÁöÑËá™ÂãïË≠òÂà•ÂíåËºâÂÖ•Ôºö\n",
    "- **REAPrÊ†ºÂºè**: `LineID`, `Type`, `Score` Ê¨Ñ‰Ωç\n",
    "- **ÈÄöÁî®Ê†ºÂºè**: `line_id`, `prediction`/`is_malicious`/`label` Ê¨Ñ‰Ωç\n",
    "- Ëá™ÂãïÁØ©ÈÅ∏È´òÂàÜÊàñÊ®ôË®òÁÇ∫ÊÉ°ÊÑèÁöÑÊ¢ùÁõÆ\n",
    "\n",
    "### 3. Êô∫ÊÖßÂ∫èÂàóÂÅµÊ∏¨ (Intelligent Sequence Detection)\n",
    "Ë™øÁî® `find_sequence_groups()` Âü∑Ë°åÈÄ≤ÈöéÁöÑÊîªÊìäÊ®°ÂºèÂàÜÊûêÔºö\n",
    "\n",
    "#### ÂÅµÊ∏¨ÂèÉÊï∏ (Detection Parameters)\n",
    "- **ÊúÄÂ§ßÈñìÈöî (max_gap)**: 1 (Â∫èÂàó‰∏≠ÂÖÅË®±ÁöÑÊìç‰ΩúÈñìÈöî)\n",
    "- **Ê®°ÂºèÂ∫´**: ‰ΩøÁî®È†êÂÆöÁæ©ÁöÑ `ATTACK_SEQUENCE_PATTERNS`\n",
    "- **ÁõÆÊ®ôÊ™îÊ°à**: Áî®ÊñºÁâπÂÆöÁöÑÂÅµÈåØÂíåÊúÄ‰Ω≥Âåñ\n",
    "\n",
    "#### ÂÅµÊ∏¨ÁµêÊûúÂ±ïÁ§∫ (Detection Results Display)\n",
    "Êèê‰æõÂâç5ÂÄãÂÅµÊ∏¨Âà∞ÁöÑÁæ§ÁµÑÁöÑË©≥Á¥∞Ë≥áË®äÔºö\n",
    "- Áæ§ÁµÑË≠òÂà•Á¢ºÂíåÊ®°ÂºèÂêçÁ®±\n",
    "- ÁΩÆ‰ø°Â∫¶ÂàÜÊï∏ÂíåÂÆåÊï¥ÊÄßË©ï‰º∞\n",
    "- ÂØ¶ÈöõÂåπÈÖçÁöÑÊìç‰ΩúËàáÈ†êÊúüÊ®°ÂºèÁöÑÂ∞çÊØî\n",
    "- Ë¶ñË¶∫ÂåñÈ°èËâ≤ÂàÜÈÖç\n",
    "\n",
    "### 4. ÈÄ≤ÈöéË¶ñË¶∫ÂåñÁîüÊàê (Advanced Visualization Generation)\n",
    "Ë™øÁî® `create_sequence_grouped_visualization()` ÂâµÂª∫Â∞àÈñÄÁöÑÂ∫èÂàóÂàÜÊûêË¶ñË¶∫ÂåñÔºö\n",
    "\n",
    "#### Ë¶ñË¶∫ÂåñÁâπËâ≤ (Visualization Features)\n",
    "- **Ê®°ÂºèÈ°èËâ≤Á∑®Á¢º**: ÊØèÁ®ÆÊîªÊìäÊ®°Âºè‰ΩøÁî®Â∞àÂ±¨È°èËâ≤\n",
    "- **Â∫èÂàóÁæ§ÁµÑÊ®ôË®ò**: Áõ∏ÂêåÂ∫èÂàóÁöÑÈÇä‰ΩøÁî®Áµ±‰∏ÄÈ°èËâ≤\n",
    "- **ÊÉ°ÊÑèË°åÁÇ∫È´ò‰∫Æ**: Êï¥ÂêàÈ†êÊ∏¨Ë≥áÊñôÁöÑÊÉ°ÊÑèÊ®ôË®ò\n",
    "- **Êú™ÂàÜÈ°ûÈÇäËôïÁêÜ**: ÁÅ∞Ëâ≤È°ØÁ§∫‰∏çÂ±¨Êñº‰ªª‰ΩïÊ®°ÂºèÁöÑÈÇä\n",
    "\n",
    "### 5. Áµ±Ë®àÂàÜÊûêËàáÂ†±Âëä (Statistical Analysis and Reporting)\n",
    "\n",
    "#### Ë¶ÜËìãÁéáÁµ±Ë®à (Coverage Statistics)\n",
    "- **Áæ§ÁµÑÂåñË¶ÜËìãÁéá**: Ë®àÁÆóË¢´Ê≠∏È°ûÂà∞ÊîªÊìäÊ®°ÂºèÁöÑÈÇäÁöÑÁôæÂàÜÊØî\n",
    "- **Êú™Áæ§ÁµÑÂåñÈÇäÊï∏**: Áµ±Ë®àÊú™ÂåπÈÖç‰ªª‰ΩïÊ®°ÂºèÁöÑÈÇä\n",
    "- **Êï¥È´îÂàÜÊûêÊïàÊûú**: Ë©ï‰º∞Ê®°ÂºèÂÅµÊ∏¨ÁöÑÊàêÊïà\n",
    "\n",
    "#### Ê®°ÂºèÂàÜ‰ΩàÂ†±Âëä (Pattern Distribution Report)\n",
    "- **ÊåâÊ®°ÂºèÁµ±Ë®à**: ÊØèÁ®ÆÊîªÊìäÊ®°ÂºèÂåπÈÖçÁöÑÈÇäÊï∏Èáè\n",
    "- **ÈáçË¶ÅÊÄßÊéíÂ∫è**: ÊåâÈÇäÊï∏ÈáèÊéíÂ∫èÁöÑÊ®°ÂºèÈáçË¶ÅÊÄß\n",
    "- **ÁôæÂàÜÊØîÂàÜÊûê**: ÂêÑÊ®°ÂºèÂú®Êï¥È´îÊ¥ªÂãï‰∏≠ÁöÑÂç†ÊØî\n",
    "\n",
    "### 6. Áï∞Â∏∏ËôïÁêÜËàáÈ©óË≠â (Exception Handling and Validation)\n",
    "- ÂÆåÊï¥ÁöÑÈåØË™§ÊçïÊçâÂíåÂ†±ÂëäÊ©üÂà∂\n",
    "- Ê™îÊ°àÂ≠òÂú®ÊÄßÁöÑÂ§öÈáçÈ©óË≠â\n",
    "- Ë©≥Á¥∞ÁöÑÂ†ÜÁñäËøΩËπ§Ë≥áË®äÁî®ÊñºÂÅµÈåØ\n",
    "\n",
    "## ÂàÜÊûêÂÉπÂÄº (Analytical Value)\n",
    "\n",
    "### ÂÆâÂÖ®ÂàÜÊûêÊáâÁî® (Security Analysis Applications)\n",
    "- **Â®ÅËÑÖÁã©Áçµ (Threat Hunting)**: Ë≠òÂà•Ë§áÈõúÁöÑÊîªÊìäÈèà\n",
    "- **‰∫ã‰ª∂ÂõûÊáâ (Incident Response)**: Âø´ÈÄüÁêÜËß£ÊîªÊìäÂ∫èÂàó\n",
    "- **Ê®°ÂºèÂ≠∏Áøí (Pattern Learning)**: ÁôºÁèæÊñ∞ÁöÑÊîªÊìäË°åÁÇ∫Ê®°Âºè\n",
    "- **È¢®Èö™Ë©ï‰º∞ (Risk Assessment)**: ÈáèÂåñÊîªÊìäÊ¥ªÂãïÁöÑÂö¥ÈáçÁ®ãÂ∫¶\n",
    "\n",
    "### Ë¶ñË¶∫ÂåñÂÑ™Âã¢ (Visualization Advantages)\n",
    "- **Áõ¥ËßÄÁêÜËß£**: Ë§áÈõúÁöÑÊó•Ë™åË≥áÊñôËΩâÂåñÁÇ∫Ë¶ñË¶∫ÂåñÂúñÂΩ¢\n",
    "- **‰∫íÂãïÊé¢Á¥¢**: ÊîØÊè¥ÂãïÊÖãÁØ©ÈÅ∏ÂíåË©≥Á¥∞Êü•Áúã\n",
    "- **Ê®°ÂºèË≠òÂà•**: Ëá™ÂãïÊ®ôË®òÂíåÂàÜÈ°ûÊîªÊìäË°åÁÇ∫\n",
    "- **ÈóúËÅØÂàÜÊûê**: Â±ïÁ§∫‰∫ã‰ª∂‰πãÈñìÁöÑÊôÇÈñìÂíåÈÇèËºØÈóú‰øÇ\n",
    "\n",
    "ÈÄôÂÄãÊ∑±Â∫¶ÂàÜÊûêÂäüËÉΩÁÇ∫Á∂≤Ë∑ØÂÆâÂÖ®Â∞àÂÆ∂Êèê‰æõ‰∫ÜÂº∑Â§ßÁöÑÂ∑•ÂÖ∑ÔºåËÉΩÂ§†ÂæûÂ§ßÈáèÁöÑÁ≥ªÁµ±Êó•Ë™å‰∏≠Âø´ÈÄüË≠òÂà•ÂíåÂàÜÊûêÊΩõÂú®ÁöÑÂÆâÂÖ®Â®ÅËÑÖ„ÄÇ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequence Grouping\n",
    "\n",
    "target_file = \"cfb61005899996469ae3023796792ca5\"  # Change this to your target file\n",
    "graph_file = os.path.join(GRAPH_PATH, target_file, f\"{target_file}.pkl\")\n",
    "\n",
    "\n",
    "graph_basename = os.path.basename(os.path.dirname(graph_file))\n",
    "graph_dir = os.path.dirname(graph_file)\n",
    "edge_metadata_file = os.path.join(graph_dir, f\"{graph_basename}_edge_metadata.json\")\n",
    "predictions_file = f\"Caldera_Ability_Predictions/{graph_basename}.csv\"\n",
    "\n",
    "if os.path.exists(graph_file) and os.path.exists(edge_metadata_file):\n",
    "    \n",
    "    # Load the graph and metadata\n",
    "    G = pickle.load(open(graph_file, \"rb\"))\n",
    "    with open(edge_metadata_file, \"r\") as fp:\n",
    "        edge_metadata_json = json.load(fp)\n",
    "    \n",
    "    # Convert JSON edge metadata back to the expected format\n",
    "    edge_metadata = {}\n",
    "    for edge_id, data in edge_metadata_json.items():\n",
    "        parts = edge_id.split('‚Üí')\n",
    "        if len(parts) == 2:\n",
    "            src = parts[0]\n",
    "            dst_key = parts[1].split('#')\n",
    "            if len(dst_key) == 2:\n",
    "                dst = dst_key[0]\n",
    "                key = int(dst_key[1]) if dst_key[1].isdigit() else dst_key[1]\n",
    "                edge_metadata[(src, dst, key)] = data\n",
    "    \n",
    "    # Load predictions if available\n",
    "    MALICIOUS_SPECS = []\n",
    "    if os.path.exists(predictions_file):\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        if 'LineID' in predictions_df.columns and 'Type' in predictions_df.columns and 'Score' in predictions_df.columns:\n",
    "            malicious_rows = predictions_df[predictions_df['Score'] >= 1.0]\n",
    "            for _, row in malicious_rows.iterrows():\n",
    "                line_id = str(row['LineID'])\n",
    "                prediction_type = row['Type'].lower()\n",
    "                if prediction_type in ['src', 'dst']:\n",
    "                    MALICIOUS_SPECS.append((line_id, prediction_type))\n",
    "                else:\n",
    "                    MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "\n",
    "    \n",
    "    sequence_groups = find_sequence_groups(\n",
    "        G, edge_metadata, ATTACK_SEQUENCE_PATTERNS, \n",
    "        max_gap=1, target_file=target_file\n",
    "    )\n",
    "    \n",
    "    # Show details of detected groups\n",
    "    if sequence_groups:\n",
    "        print(f\"\\nüìã DETECTED SEQUENCE GROUPS:\")\n",
    "        for group_id, group_info in list(sequence_groups.items())[:5]:  # Show first 5 groups\n",
    "            print(f\"\\n   Group {group_id}: {group_info['pattern'].name}\")\n",
    "            print(f\"      Description: {group_info['pattern'].description}\")\n",
    "            print(f\"      Confidence: {group_info['confidence']:.2f}\")\n",
    "            print(f\"      Edges: {len(group_info['edges'])}\")\n",
    "            print(f\"      Operations: {', '.join(group_info['matched_operations'][:3])}{'...' if len(group_info['matched_operations']) > 3 else ''}\")\n",
    "            print(f\"      Expected pattern: {', '.join(group_info['pattern'].operations)}\")\n",
    "            print(f\"      Strict order: {group_info['pattern'].strict_order}\")\n",
    "            print(f\"      Completeness: {group_info.get('completeness', 0):.2f}\")\n",
    "            print(f\"      Color: {group_info['pattern'].color}\")\n",
    "\n",
    "    try:\n",
    "        output_path = os.path.join(graph_dir, f\"sequence_grouped_{graph_basename}.html\")\n",
    "        viz_path, groups = create_sequence_grouped_visualization(\n",
    "            G, edge_metadata, MALICIOUS_SPECS, ATTACK_SEQUENCE_PATTERNS, output_path, sequence_groups\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Summary of what was found\n",
    "        if groups:\n",
    "            total_grouped_edges = sum(len(g['edges']) for g in groups.values())\n",
    "            total_edges = G.number_of_edges()\n",
    "            coverage = (total_grouped_edges / total_edges) * 100\n",
    "            \n",
    "            print(f\"\\nüìä Sequence Coverage:\")\n",
    "            print(f\"   Grouped edges: {total_grouped_edges}/{total_edges} ({coverage:.1f}%)\")\n",
    "            print(f\"   Ungrouped edges: {total_edges - total_grouped_edges} (shown in gray)\")\n",
    "            \n",
    "            # Show pattern distribution\n",
    "            pattern_counts = defaultdict(int)\n",
    "            for group_info in groups.values():\n",
    "                pattern_counts[group_info['pattern'].name] += len(group_info['edges'])\n",
    "            \n",
    "            print(f\"\\nüéØ Pattern Distribution:\")\n",
    "            for pattern_name, edge_count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (edge_count / total_edges) * 100\n",
    "                print(f\"   {pattern_name}: {edge_count} edges ({percentage:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error creating sequence visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"‚ùå Required files not found for {target_file}\")\n",
    "    print(f\"   Graph file: {graph_file}\")\n",
    "    print(f\"   Metadata file: {edge_metadata_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
