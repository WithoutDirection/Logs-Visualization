{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3218169c",
   "metadata": {},
   "source": [
    "# 日誌視覺化與攻擊序列分析系統 (Log Visualization and Attack Sequence Analysis System)\n",
    "\n",
    "本專案用於處理 Caldera 攻擊模擬平台產生的日誌資料，將其轉換為網路圖(Graph)並進行視覺化分析。主要功能包括：\n",
    "\n",
    "1. **資料預處理**: 將 CSV 格式的日誌檔案轉換為標準化格式\n",
    "2. **圖形建構**: 基於日誌事件建立節點和邊的關係圖\n",
    "3. **攻擊序列偵測**: 識別預定義的攻擊模式和行為序列\n",
    "4. **互動式視覺化**: 產生可互動的 HTML 圖形介面，支援時間篩選和序列分析\n",
    "\n",
    "## 系統架構 (System Architecture)\n",
    "- **輸入**: Caldera_Ability_Statistics/ 目錄中的 CSV 日誌檔案(由 Procmon 錄製)\n",
    "- **處理**: 使用 NetworkX 建立圖形結構，並應用攻擊模式比對演算法\n",
    "- **輸出**: 儲存為 pickle 格式的圖形檔案和互動式 HTML 視覺化介面"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a00fa00b",
   "metadata": {},
   "source": [
    "# 套件匯入與初始設定 (Package Imports and Initial Configuration)\n",
    "\n",
    "此單元格匯入所有必要的 Python 套件和模組：\n",
    "\n",
    "## 核心套件 (Core Packages)\n",
    "- **glob, os**: 檔案系統操作和路徑處理\n",
    "- **networkx**: 圖形資料結構的建立與操作\n",
    "- **pandas**: 資料處理和 CSV 檔案讀取\n",
    "- **matplotlib**: 基礎圖形繪製功能\n",
    "\n",
    "## 專用工具 (Specialized Tools)\n",
    "- **tqdm**: 進度條顯示，用於長時間運行的迴圈\n",
    "- **natsort**: 自然排序，確保檔案按正確順序處理\n",
    "- **pickle**: 物件序列化，用於儲存圖形結構\n",
    "- **uuid**: 產生唯一識別碼，用於節點標識\n",
    "\n",
    "## 自定義模組 (Custom Modules)\n",
    "- **graphutil**: 包含圖形生成和視覺化的核心函數\n",
    "\n",
    "設定輸出目錄 GRAPH_PATH = \"Graphs\" 用於儲存處理後的圖形檔案。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "426d4ce0",
   "metadata": {},
   "source": [
    "# Define packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e0785e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "from tqdm import tqdm\n",
    "import networkx as nx\n",
    "from natsort import natsorted\n",
    "import json\n",
    "import uuid\n",
    "import os\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "from collections import defaultdict, namedtuple\n",
    "\n",
    "from graphutil import *\n",
    "\n",
    "GRAPH_PATH = \"Graphs\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e3abc1",
   "metadata": {},
   "source": [
    "# 攻擊序列模式定義 (Attack Sequence Pattern Definition)\n",
    "\n",
    "本單元格定義了系統用於識別攻擊行為的預設模式。每個模式使用 `SequencePattern` 命名元組(namedtuple)結構，包含以下屬性：\n",
    "\n",
    "## 模式屬性說明 (Pattern Attributes)\n",
    "- **name**: 模式名稱，用於識別攻擊類型\n",
    "- **operations**: 操作序列列表，定義該攻擊模式包含的系統操作\n",
    "- **color**: 視覺化時使用的顏色代碼\n",
    "- **description**: 模式的文字描述\n",
    "- **min_length**: 最小序列長度要求\n",
    "- **strict_order**: 是否要求嚴格的操作順序\n",
    "- **results**: 預期的操作結果狀態\n",
    "\n",
    "## 預定義攻擊模式 (Predefined Attack Patterns)\n",
    "\n",
    "### 1. 程序創建模式 (Process Creation)\n",
    "- 偵測新程序的創建行為\n",
    "- 顏色：紅色 (#FF3333)\n",
    "\n",
    "### 2. 檔案操作模式 (File Manipulation)\n",
    "- **File_Creation_Write**: 檔案創建後進行寫入\n",
    "- **File_Creation_Metadata_Write**: 包含元資料查詢的完整檔案操作\n",
    "- 顏色：橙色/黃色系\n",
    "\n",
    "### 3. 註冊表操作模式 (Registry Manipulation)\n",
    "- **Registry_Creation_Modification**: 註冊表鍵值的創建和修改\n",
    "- **Registry_Modification**: 現有註冊表鍵值的修改\n",
    "- 顏色：青色/綠色系\n",
    "\n",
    "### 4. 網路通訊模式 (Network Communication)\n",
    "- **TCP_Communication**: TCP 連接的完整生命週期\n",
    "- 包含連接、發送、接收、斷開等步驟\n",
    "- 顏色：藍色 (#0066FF)\n",
    "\n",
    "這些模式將在後續的圖形分析中用於自動識別和標記可疑的攻擊行為序列。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "540d4bc9",
   "metadata": {},
   "source": [
    "# Rules for the sequence matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce1bfab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predefined attack sequence patterns based on exact operation names\n",
    "SequencePattern = namedtuple('SequencePattern', ['name', 'operations', 'color', 'description', 'min_length', 'strict_order', 'results'])\n",
    "\n",
    "\n",
    "ATTACK_SEQUENCE_PATTERNS = [\n",
    "    # Process creation patterns\n",
    "    SequencePattern(\n",
    "        name=\"Process_Creation\",\n",
    "        operations=[\"Process Create\"],\n",
    "        color=\"#FF3333\",  # Red\n",
    "        description=\"Process creation operations\",\n",
    "        min_length=1,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\"]  # Expected results for this pattern\n",
    "    ),\n",
    "\n",
    "    # File manipulation patterns\n",
    "    SequencePattern(\n",
    "        name=\"File_Creation_Write\",\n",
    "        operations=[\"CreateFile\", \"WriteFile\"],\n",
    "        color=\"#FF6600\",  # Orange\n",
    "        description=\"File creation followed by write operations\",\n",
    "        min_length=2,\n",
    "        strict_order=True,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "    SequencePattern(\n",
    "        name=\"File_Creation_Metadata_Write\",\n",
    "        operations=[\"CreateFile\", \"QueryBasicInformationFile\", \"WriteFile\", \"CloseFile\"],\n",
    "        color=\"#FAD000\",  # Yellow\n",
    "        description=\"File creation with metadata query and write\",\n",
    "        min_length=1,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    # Registry manipulation patterns\n",
    "    SequencePattern(\n",
    "        name=\"Registry_Creation_Modification\",\n",
    "        operations=[\"RegCreateKey\", \"RegSetValue\", \"RegQueryKey\", \"RegCloseKey\"],\n",
    "        color=\"#34CCFF\",  # Cyan\n",
    "        description=\"Registry key Create and modification\",\n",
    "        min_length=2,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    SequencePattern(\n",
    "        name=\"Registry_Modification\",\n",
    "        operations=[\"RegOpenKey\", \"RegSetValue\", \"RegQueryKey\", \"RegCloseKey\"],\n",
    "        color=\"#33CC33\",  # Green\n",
    "        description=\"Registry key open and modification\",\n",
    "        min_length=2,\n",
    "        strict_order=False,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"]  \n",
    "    ),\n",
    "\n",
    "    # Network communication patterns\n",
    "    SequencePattern(\n",
    "        name=\"TCP_Communication\",\n",
    "        operations=[\"TCP Connect\", \"TCP Send\", \"TCP Receive\", \"TCP Disconnect\"],\n",
    "        color=\"#0066FF\",  # Blue\n",
    "        description=\"TCP network communication sequence\",\n",
    "        min_length=2,\n",
    "        strict_order=True,\n",
    "        results=[\"SUCCESS\", \"SUCCESS\", \"SUCCESS\", \"SUCCESS\"] \n",
    "    ),\n",
    "    \n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec388f28",
   "metadata": {},
   "source": [
    "# 日誌檔案轉換為圖形結構 (Convert Log Files to Graph Structure)\n",
    "\n",
    "此區塊負責將 CSV 格式的日誌檔案轉換為網路圖(NetworkX Graph)結構，是整個系統的核心資料處理步驟。\n",
    "\n",
    "## 主要處理流程 (Main Processing Flow)\n",
    "\n",
    "### 1. 檔案探索與驗證 (File Discovery and Validation)\n",
    "- 搜尋 `Caldera_Ability_Statistics/` 目錄中的 CSV 檔案\n",
    "- 優先處理已有 `_raw_events_with_lineid.csv` 後綴的檔案\n",
    "- 若不存在，則自動為原始 CSV 檔案添加 `lineid` 欄位\n",
    "\n",
    "### 2. 編碼處理 (Encoding Handling)\n",
    "系統嘗試多種字元編碼以確保相容性：\n",
    "- UTF-8 (標準編碼)\n",
    "- UTF-8-BOM (包含位元組順序標記)\n",
    "- CP950 (繁體中文 Big5 編碼)\n",
    "\n",
    "### 3. 資料結構轉換 (Data Structure Conversion)\n",
    "將 CSV 資料列轉換為 `campaign_events` 格式，包含：\n",
    "- **srcNode**: 來源節點資訊 (通常為程序)\n",
    "- **dstNode**: 目標節點資訊 (檔案、註冊表、網路等)\n",
    "- **relation**: 操作關係類型\n",
    "- **timestamp**: 時間戳記\n",
    "- **line_id**: 原始日誌行號\n",
    "\n",
    "### 4. 節點類型分類 (Node Type Classification)\n",
    "根據事件類別自動分類節點：\n",
    "- **Process**: 程序節點，包含 PID 和程序名稱\n",
    "- **File**: 檔案系統節點，包含路徑資訊\n",
    "- **Registry**: 註冊表節點，包含鍵值路徑\n",
    "- **Network**: 網路節點，包含 IP 位址資訊\n",
    "\n",
    "### 5. UUID 生成策略 (UUID Generation Strategy)\n",
    "使用 UUID5 (基於命名空間的雜湊) 確保相同資源的唯一性：\n",
    "- 程序: `程序名稱_PID`\n",
    "- 檔案: `檔案路徑`\n",
    "- 註冊表: `註冊表鍵值路徑`\n",
    "- 網路: `IP位址`\n",
    "\n",
    "這種方法確保了圖形中相同資源的節點會被正確合併，避免重複節點的產生。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd45c98e",
   "metadata": {},
   "source": [
    "# Convert all logs into graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd45c98e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# caldera_dir = os.path.join(os.getcwd(), 'Caldera_Ability_Statistics')\n",
    "# for file in os.listdir(caldera_dir):\n",
    "#     if file.endswith(\"_raw_events_with_lineid.csv\"):\n",
    "#         # remove the old processed files\n",
    "#         os.remove(os.path.join(caldera_dir, file))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2263895f",
   "metadata": {},
   "source": [
    "# 檔案準備與預處理 (File Preparation and Preprocessing)\n",
    "\n",
    "此區塊負責準備和預處理輸入的 CSV 日誌檔案，確保資料格式的統一性。\n",
    "\n",
    "## 處理邏輯 (Processing Logic)\n",
    "\n",
    "### 1. 檔案搜尋策略 (File Search Strategy)\n",
    "- **優先級 1**: 尋找已存在的 `*_raw_events_with_lineid.csv` 檔案\n",
    "- **優先級 2**: 若無預處理檔案，則處理原始 CSV 檔案並添加行號\n",
    "\n",
    "### 2. 自動格式轉換 (Automatic Format Conversion)\n",
    "當找不到預處理檔案時，系統會：\n",
    "- 掃描 `Caldera_Ability_Statistics/` 目錄\n",
    "- 為每個原始 CSV 檔案添加 `lineid` 欄位\n",
    "- 處理多種字元編碼問題 (UTF-8, UTF-8-BOM, CP1252, ISO-8859-1, CP950)\n",
    "- 將轉換後的檔案儲存為 `*_raw_events_with_lineid.csv` 格式\n",
    "\n",
    "### 3. 錯誤處理機制 (Error Handling)\n",
    "- 自動嘗試多種編碼格式\n",
    "- 跳過無法解碼的檔案並記錄錯誤\n",
    "- 提供詳細的處理進度回饋\n",
    "\n",
    "### 4. 資料完整性檢查 (Data Integrity Check)\n",
    "- 驗證必要欄位的存在性\n",
    "- 自動添加缺失的 `lineid` 欄位\n",
    "- 確保資料格式符合後續處理需求\n",
    "\n",
    "這個預處理步驟確保了所有輸入檔案都具有統一的格式，包含必要的行號資訊，為後續的圖形建構提供可靠的資料基礎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11064976",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look for CSV files that match the attached dataset pattern\n",
    "csv_files = natsorted(glob(os.path.join(os.getcwd(), 'Caldera_Ability_Statistics/*_raw_events_with_lineid.csv')))\n",
    "if csv_files:\n",
    "    files = csv_files\n",
    "    print(f\"Found existing *_raw_events_with_lineid.csv files: {len(files)}\")\n",
    "else:\n",
    "    # create *_raw_events_with_lineid.csv list from directory\n",
    "    print(\"No *_raw_events_with_lineid.csv files found. Converting existing CSV files...\")\n",
    "    files = []\n",
    "    \n",
    "    caldera_dir = os.path.join(os.getcwd(), 'Caldera_Ability_Statistics')\n",
    "    if not os.path.exists(caldera_dir):\n",
    "        os.makedirs(caldera_dir)\n",
    "        print(f\"Created directory: {caldera_dir}\")\n",
    "        print(\"Please place your CSV files in this directory and run again.\")\n",
    "    else:\n",
    "        for file in os.listdir(caldera_dir):\n",
    "            if file.endswith(\".csv\") and not file.endswith(\"_raw_events_with_lineid.csv\"):\n",
    "                original_file = os.path.join(caldera_dir, file)\n",
    "                \n",
    "                # Read the original CSV with encoding handling\n",
    "                try:\n",
    "                    # Try different encodings\n",
    "                    encodings_to_try = ['utf-8', 'utf-8-sig', 'cp1252', 'iso-8859-1', 'cp950']\n",
    "                    df = None\n",
    "                    \n",
    "                    for encoding in encodings_to_try:\n",
    "                        try:\n",
    "                            df = pd.read_csv(original_file, encoding=encoding)\n",
    "                            break\n",
    "                        except UnicodeDecodeError:\n",
    "                            continue\n",
    "                    \n",
    "                    if df is None:\n",
    "                        print(f\"  ❌ Could not decode {file} with any supported encoding\")\n",
    "                        continue\n",
    "                    \n",
    "                    # Add lineid column if it doesn't exist\n",
    "                    if 'lineid' not in df.columns and 'LineID' not in df.columns and 'line_id' not in df.columns:\n",
    "                        df['lineid'] = range(1, len(df) + 1)\n",
    "                        # print(f\"  Added lineid column to {file}\")\n",
    "                    \n",
    "                    # Generate new filename\n",
    "                    base_name = file.replace('.csv', '')\n",
    "                    new_filename = f\"{base_name}_raw_events_with_lineid.csv\"\n",
    "                    new_filepath = os.path.join(caldera_dir, new_filename)\n",
    "                    \n",
    "                    # Save the modified CSV with UTF-8 encoding\n",
    "                    df.to_csv(new_filepath, index=False, encoding='utf-8')\n",
    "                    files.append(new_filepath)\n",
    "                    print(f\"\\r✅ Created: {new_filename}\", end='', flush=True)\n",
    "                    \n",
    "                except Exception as e:\n",
    "                    print(f\"\\r❌ Error processing {file}: {e}\", end='', flush=True)\n",
    "                    continue\n",
    "        \n",
    "        if not files:\n",
    "            print(\"\\nNo CSV files found to convert. Please add CSV files to Caldera_Ability_Statistics/ directory.\")\n",
    "        else:\n",
    "            print(f\"\\nSuccessfully converted {len(files)} files to *_raw_events_with_lineid.csv format\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7a8f2f",
   "metadata": {},
   "source": [
    "# 批次圖形生成與儲存 (Batch Graph Generation and Storage)\n",
    "\n",
    "此核心處理單元格負責將預處理的 CSV 檔案轉換為 NetworkX 圖形結構，並儲存相關的中繼資料。\n",
    "\n",
    "## 主要處理步驟 (Main Processing Steps)\n",
    "\n",
    "### 1. 檔案迴圈處理 (File Loop Processing)\n",
    "- 使用 `tqdm` 顯示整體處理進度\n",
    "- 逐一處理每個 `*_raw_events_with_lineid.csv` 檔案\n",
    "- 自動跳過無法解碼的檔案\n",
    "\n",
    "### 2. CSV 資料解析 (CSV Data Parsing)\n",
    "對每個 CSV 檔案執行以下操作：\n",
    "- **多編碼嘗試**: UTF-8, UTF-8-BOM, CP950\n",
    "- **欄位對應**: 將 CSV 欄位對應到標準化的事件結構\n",
    "- **UUID 生成**: 為每個資源生成唯一標識符\n",
    "\n",
    "### 3. 事件分類與節點建立 (Event Classification and Node Creation)\n",
    "\n",
    "#### 檔案系統事件 (File System Events)\n",
    "- 識別關鍵字: 'file', 'file system'\n",
    "- 節點屬性: UUID, Name, Type=\"File\"\n",
    "- 路徑處理: 從 'Path' 或 'Content' 欄位提取\n",
    "\n",
    "#### 註冊表事件 (Registry Events)\n",
    "- 識別關鍵字: 'registry'\n",
    "- 節點屬性: UUID, Key, Type=\"Registry\"\n",
    "- 鍵值處理: 從 'Path' 或 'Detail' 欄位提取\n",
    "\n",
    "#### 網路事件 (Network Events)\n",
    "- 識別關鍵字: 'network'\n",
    "- 節點屬性: UUID, Dstaddress, Type=\"Network\"\n",
    "- IP 提取: 使用正規表達式提取 IP 位址\n",
    "\n",
    "#### 程序事件 (Process Events)\n",
    "- 識別關鍵字: 'process'\n",
    "- 特殊處理: 區分 Process Create 和 Process Start\n",
    "- PID 解析: 從 Detail 欄位或 PID 欄位提取程序 ID\n",
    "\n",
    "### 4. 時間戳記處理 (Timestamp Processing)\n",
    "嘗試多種時間格式：\n",
    "- `\"%m/%d/%Y %I:%M:%S %p\"` (美式日期時間)\n",
    "- `\"%Y-%m-%d %H:%M:%S\"` (ISO 格式)\n",
    "- 失敗時使用行號作為序列號\n",
    "\n",
    "### 5. 圖形生成與儲存 (Graph Generation and Storage)\n",
    "- 調用 `generate_query_graph()` 函數建立 NetworkX 圖形\n",
    "- 將圖形儲存為 pickle 格式 (`*.pkl`)\n",
    "- 將邊的中繼資料儲存為 JSON 格式 (`*_edge_metadata.json`)\n",
    "\n",
    "### 6. 中繼資料結構 (Metadata Structure)\n",
    "邊的中繼資料包含：\n",
    "- `line_id`: 原始日誌行號\n",
    "- `operation`: 操作類型\n",
    "- `timestamp`: 時間戳記\n",
    "- `technique`: 技術分類\n",
    "- `src_process`: 來源程序資訊\n",
    "- `src_pid`: 來源程序 ID\n",
    "- `dst_resource`: 目標資源\n",
    "- `dst_type`: 目標類型\n",
    "\n",
    "此步驟的輸出為每個日誌檔案產生一組完整的圖形資料，包括圖形結構和詳細的中繼資料，為後續的分析和視覺化提供基礎。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56cfa73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Found {len(files)} file(s) to process\")\n",
    "print(\"Building provenance graphs...\")\n",
    "\n",
    "for idx, f in enumerate(tqdm(files, desc=\"Processing files\")):\n",
    "    # print(f\"\\n📁 Processing file {idx+1}/{len(files)}: {os.path.basename(f)}\")\n",
    "    \n",
    "    if True:\n",
    "        # Convert CSV rows to the campaign_events format expected by generate_query_graph\n",
    "        import csv\n",
    "        from datetime import datetime\n",
    "        import re\n",
    "        campaign_events = []\n",
    "        \n",
    "        # Try different encodings to handle various CSV formats\n",
    "        encodings_to_try = ['utf-8', 'utf-8-sig', 'cp950']\n",
    "        \n",
    "        for encoding in encodings_to_try:\n",
    "            try:\n",
    "                with open(f, newline='', encoding=encoding) as csvfile:\n",
    "                    reader = csv.DictReader(csvfile)\n",
    "                    rows = list(reader)\n",
    "                    break  # If successful, break out of the encoding loop\n",
    "            except UnicodeDecodeError:\n",
    "                continue  # Try next encoding\n",
    "        else:\n",
    "            # If all encodings fail, skip this file\n",
    "            print(f\"❌ Could not decode file {os.path.basename(f)} with any supported encoding\")\n",
    "            continue\n",
    "            \n",
    "        for i, row in enumerate(tqdm(rows, desc=\"Converting CSV\", leave=False)):\n",
    "            src_name = row.get('Process Name') or row.get('Image Path') or row.get('User') or 'unknown'\n",
    "            pid = row.get('PID') or row.get('Parent PID') or ''\n",
    "            try:\n",
    "                pid_int = int(pid)\n",
    "            except:\n",
    "                pid_int = 0\n",
    "            src_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{src_name}_{pid_int}\"))\n",
    "            srcNode = {\"UUID\": src_uuid, \"Name\": src_name, \"Pid\": pid_int, \"Type\": \"Process\"}\n",
    "\n",
    "            event_class = (row.get('Event Class') or '').lower()\n",
    "            operation = row.get('Operation') or row.get('srcEvent') or 'unknown'\n",
    "            relation = operation\n",
    "            dstNode = None\n",
    "            if 'file' in event_class or 'file system' in event_class:\n",
    "                path = row.get('Path') or row.get('Content') or ''\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, path)) if path else src_uuid\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Name\": os.path.basename(path) if path else path, \"Type\": \"File\"}\n",
    "            elif 'registry' in event_class:\n",
    "                key = row.get('Path') or row.get('Detail') or ''\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, key)) if key else src_uuid\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Key\": key, \"Type\": \"Registry\"}\n",
    "            elif 'network' in event_class:\n",
    "                addr = ''\n",
    "                for field in ('Detail','Path','Content','srcEvent'):\n",
    "                    text = row.get(field,'')\n",
    "                    if not text:\n",
    "                        continue\n",
    "                    m = re.search(r'(?:\\b)(?:\\d{1,3}\\.){3}\\d{1,3}(?:\\b)', text)\n",
    "                    if m:\n",
    "                        addr = m.group(0); break\n",
    "                if addr:\n",
    "                    dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, addr))\n",
    "                    dstNode = {\"UUID\": dst_uuid, \"Dstaddress\": addr, \"Type\": \"Network\"}\n",
    "            elif 'process' in event_class:\n",
    "                path = row.get('Path') or row.get('Image Path') or ''\n",
    "                dst_name = os.path.basename(path.replace('\\\\', '/').strip()) if path else ''\n",
    "                dst_name = dst_name.lstrip('/\\\\').strip()\n",
    "                #print(f\"Processing process event: {dst_name} from {path}\")\n",
    "                op_lower = (operation or '').lower()\n",
    "\n",
    "                # Determine destination PID based on operation subtype\n",
    "                dst_pid = 0\n",
    "                if 'create' in op_lower:  # Process Create\n",
    "                    details_text = row.get('Detail') or row.get('Details') or ''\n",
    "                    m = re.search(r'PID\\s*:\\s*(\\d+)', details_text)\n",
    "                    if m:\n",
    "                        try:\n",
    "                            dst_pid = int(m.group(1))\n",
    "                        except:\n",
    "                            dst_pid = 0\n",
    "                    else:\n",
    "                        # Fallback to PID / Parent PID\n",
    "                        dst_pid_val = row.get('PID') or row.get('Parent PID') or 0\n",
    "                        try:\n",
    "                            dst_pid = int(dst_pid_val)\n",
    "                        except:\n",
    "                            dst_pid = 0\n",
    "                else:  # Process Start or others\n",
    "                    dst_pid_val = row.get('PID') or row.get('Parent PID') or 0\n",
    "                    try:\n",
    "                        dst_pid = int(dst_pid_val)\n",
    "                    except:\n",
    "                        dst_pid = 0\n",
    "                #print(f\"  Destination PID: {dst_pid}\")\n",
    "                dst_uuid = str(uuid.uuid5(uuid.NAMESPACE_DNS, f\"{dst_name}_{dst_pid}\"))\n",
    "                dstNode = {\"UUID\": dst_uuid, \"Name\": dst_name, \"Pid\": dst_pid, \"Type\": \"Process\"}\n",
    "\n",
    "            # timestamp parsing: try common formats, otherwise use row index\n",
    "            ts = None\n",
    "            dtstr = row.get('Date & Time') or row.get('Date') or ''\n",
    "            try:\n",
    "                ts = int(datetime.strptime(dtstr, \"%m/%d/%Y %I:%M:%S %p\").timestamp())\n",
    "            except:\n",
    "                try:\n",
    "                    ts = int(datetime.strptime(dtstr, \"%Y-%m-%d %H:%M:%S\").timestamp())\n",
    "                except:\n",
    "                    ts = i\n",
    "\n",
    "            # include line id if present in CSV\n",
    "            line_id = row.get('lineid') or row.get('LineID') or row.get('line_id') or ''\n",
    "\n",
    "            ev = {\"srcNode\": srcNode, \"dstNode\": dstNode, \"relation\": relation, \"timestamp\": ts, \"line_id\": str(line_id) if line_id != '' else None}\n",
    "            ev[\"label\"] = f\"CSV_{operation}\"\n",
    "            campaign_events.append(ev)\n",
    "\n",
    "    # print(f\"  ✅ Loaded {len(campaign_events)} events\")\n",
    "\n",
    "    eventname = os.path.basename(f).replace('_raw_events_with_lineid.csv', '')\n",
    "    save_path = os.path.join(GRAPH_PATH, eventname)\n",
    "    if not os.path.isdir(save_path):\n",
    "        os.makedirs(save_path)\n",
    "\n",
    "    G, edge_metadata = generate_query_graph(campaign_events)\n",
    "    \n",
    "\n",
    "    # Save graph and metadata\n",
    "    with open(f\"{save_path}/{eventname}.pkl\", \"wb\") as fp:\n",
    "        pickle.dump(G, fp)\n",
    "    with open(f\"{save_path}/{eventname}_edge_metadata.json\", \"w\") as fp:\n",
    "        # Convert edge metadata to JSON-serializable format\n",
    "        json_metadata = {}\n",
    "        for (src, dst, key), data in edge_metadata.items():\n",
    "            edge_id = f\"{src}→{dst}#{key}\"\n",
    "            json_metadata[edge_id] = {\n",
    "                'line_id': data['line_id'],\n",
    "                'operation': data['operation'],\n",
    "                'timestamp': data['timestamp'],\n",
    "                'technique': data['technique'],\n",
    "                'src_process': data['src_process'],\n",
    "                'src_pid': data['src_pid'],\n",
    "                'dst_resource': data['dst_resource'],\n",
    "                'dst_type': data['dst_type']\n",
    "            }\n",
    "        json.dump(json_metadata, fp, indent=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "462dae60",
   "metadata": {},
   "source": [
    "# 圖形檔案蒐集與驗證 (Graph File Collection and Validation)\n",
    "\n",
    "此單元格負責蒐集所有已生成的圖形檔案，並驗證其完整性，為後續的視覺化處理做準備。\n",
    "\n",
    "## 功能說明 (Functionality Description)\n",
    "\n",
    "### 1. 目錄結構掃描 (Directory Structure Scanning)\n",
    "- 掃描 `GRAPH_PATH` (預設為 \"Graphs\") 目錄下的所有子目錄\n",
    "- 每個子目錄對應一個處理過的日誌檔案\n",
    "- 目錄名稱通常為原始檔案的基礎名稱 (不含副檔名)\n",
    "\n",
    "### 2. Pickle 檔案驗證 (Pickle File Validation)\n",
    "- 在每個子目錄中尋找 `{eventname}.pkl` 檔案\n",
    "- 這些 pickle 檔案包含完整的 NetworkX 圖形物件\n",
    "- 只有存在對應 pickle 檔案的目錄才會被加入處理清單\n",
    "\n",
    "### 3. 可用圖形登錄 (Available Graphs Registration)\n",
    "- 將找到的有效圖形檔案路徑加入 `available_graphs` 清單\n",
    "- 提供即時的發現回饋，顯示找到的圖形檔案\n",
    "- 為後續的批次視覺化處理建立檔案清單\n",
    "\n",
    "### 4. 品質保證 (Quality Assurance)\n",
    "此步驟確保：\n",
    "- 所有圖形檔案都是完整且可讀取的\n",
    "- 避免處理不完整或損壞的資料\n",
    "- 提供清楚的處理範圍資訊\n",
    "\n",
    "這個驗證步驟是批次視覺化處理的重要前置作業，確保後續處理的可靠性和效率。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ed452c5",
   "metadata": {},
   "source": [
    "# Gather all graphs for visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c69c481e",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_base_dir = GRAPH_PATH  \n",
    "available_graphs = []\n",
    "\n",
    "# Look for .pkl files in subdirectories of Graphs/\n",
    "for subdir in os.listdir(graph_base_dir):\n",
    "    subdir_path = os.path.join(graph_base_dir, subdir)\n",
    "    if os.path.isdir(subdir_path):\n",
    "        # Look for {eventname}.pkl in each subdirectory\n",
    "        pkl_file = os.path.join(subdir_path, f\"{subdir}.pkl\")\n",
    "        if os.path.exists(pkl_file):\n",
    "            available_graphs.append(pkl_file)\n",
    "            print(f\"\\rFound graph: {pkl_file}\", end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28303cec",
   "metadata": {},
   "source": [
    "# 互動式視覺化生成 (Interactive Visualization Generation)\n",
    "\n",
    "此單元格為所有可用的圖形生成互動式的 HTML 視覺化介面。目前專注於基於條目範圍的互動式視覺化。\n",
    "\n",
    "## 處理流程 (Processing Workflow)\n",
    "\n",
    "### 1. 批次處理設置 (Batch Processing Setup)\n",
    "- 處理所有在上一步驟中找到的有效圖形檔案\n",
    "- 初始化成功和失敗的視覺化計數器\n",
    "- 提供詳細的處理進度報告\n",
    "\n",
    "### 2. 必要檔案檢查 (Required Files Validation)\n",
    "對每個圖形檔案，系統驗證以下檔案的存在：\n",
    "- **圖形檔案**: `{eventname}.pkl` (NetworkX 圖形物件)\n",
    "- **中繼資料檔案**: `{eventname}_edge_metadata.json` (邊的詳細資訊)\n",
    "- **預測檔案**: `Caldera_Ability_Predictions/{eventname}.csv` (可選的惡意行為預測)\n",
    "\n",
    "### 3. 時間範圍分析 (Time Range Analysis)\n",
    "- 使用 `get_graph_time_range()` 函數分析圖形的時間跨度\n",
    "- 計算最小和最大時間戳記\n",
    "- 跳過沒有時間資訊的圖形\n",
    "\n",
    "### 4. 惡意行為預測整合 (Malicious Behavior Prediction Integration)\n",
    "支援多種預測檔案格式：\n",
    "\n",
    "#### 格式 A: LineID + Type + Score\n",
    "- 欄位: `LineID`, `Type`, `Score`\n",
    "- 條件: `Score >= 1.0`\n",
    "- 類型: `src`, `dst`, 或 `both`\n",
    "\n",
    "#### 格式 B: line_id + prediction/label\n",
    "- 欄位: `line_id`, `prediction`/`is_malicious`/`label`\n",
    "- 條件: 預測值為 1 或標籤為 'malicious'\n",
    "\n",
    "### 5. 視覺化生成 (Visualization Generation)\n",
    "- 調用 `create_interactive_entry_visualization()` 生成互動式 HTML\n",
    "- 支援基於條目序號的篩選功能\n",
    "- 整合惡意行為標記 (如果有預測資料)\n",
    "\n",
    "### 6. 統計資訊收集 (Statistics Collection)\n",
    "為每個成功的視覺化收集：\n",
    "- **節點數量**: 圖形中的節點總數\n",
    "- **邊數量**: 圖形中的邊總數\n",
    "- **時間跨度**: 事件的時間範圍\n",
    "- **惡意標記數量**: REAPr 或其他預測系統標記的惡意行為數量\n",
    "- **預測可用性**: 是否有可用的預測資料\n",
    "\n",
    "### 7. 結果報告 (Results Reporting)\n",
    "生成詳細的處理報告，包括：\n",
    "- 成功處理的視覺化數量和詳細資訊\n",
    "- 失敗的視覺化及其原因\n",
    "- 每個圖形的統計摘要表格\n",
    "\n",
    "## 注意事項 (Notes)\n",
    "- **時間視覺化**: 目前已停用時間範圍篩選功能，因為在大多數情況下用處有限\n",
    "- **條目視覺化**: 專注於基於日誌條目順序的互動式分析\n",
    "- **錯誤處理**: 自動跳過有問題的檔案並記錄錯誤原因\n",
    "\n",
    "此功能為分析師提供了強大的互動式工具，可以深入探索攻擊序列和系統行為模式。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54101dcf",
   "metadata": {},
   "source": [
    "# Generate visualization that can be filtered by time and entry range\n",
    "\n",
    "Time range is disabled for now since its almost useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a32a7a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create Interactive Time Visualizations for ALL Available Graphs\n",
    "\n",
    "print(f\"🔍 Creating interactive time visualizations for {len(available_graphs)} graphs\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "successful_visualizations = []\n",
    "failed_visualizations = []\n",
    "\n",
    "for graph_file in available_graphs:\n",
    "    graph_basename = os.path.basename(os.path.dirname(graph_file))\n",
    "    graph_dir = os.path.dirname(graph_file)\n",
    "    edge_metadata_file = os.path.join(graph_dir, f\"{graph_basename}_edge_metadata.json\")\n",
    "    predictions_file = f\"Caldera_Ability_Predictions/{graph_basename}.csv\"\n",
    "    \n",
    "    print(f\"\\n📊 Processing: {graph_basename}\")\n",
    "    \n",
    "    # Check if required files exist\n",
    "    if not os.path.exists(edge_metadata_file):\n",
    "        print(f\"❌ Missing edge metadata file - skipping\")\n",
    "        failed_visualizations.append((graph_basename, \"Missing metadata\"))\n",
    "        continue\n",
    "    \n",
    "    try:\n",
    "        # Load the graph and metadata\n",
    "        G = pickle.load(open(graph_file, \"rb\"))\n",
    "        with open(edge_metadata_file, \"r\") as fp:\n",
    "            edge_metadata_json = json.load(fp)\n",
    "        \n",
    "        # Convert edge metadata\n",
    "        edge_metadata = {}\n",
    "        for edge_id, data in edge_metadata_json.items():\n",
    "            parts = edge_id.split('→')\n",
    "            if len(parts) == 2:\n",
    "                src = parts[0]\n",
    "                dst_key = parts[1].split('#')\n",
    "                if len(dst_key) == 2:\n",
    "                    dst = dst_key[0]\n",
    "                    key = int(dst_key[1]) if dst_key[1].isdigit() else dst_key[1]\n",
    "                    edge_metadata[(src, dst, key)] = data\n",
    "        \n",
    "        # Check if graph has timestamps\n",
    "        min_ts, max_ts = get_graph_time_range(G, edge_metadata)\n",
    "        if min_ts is None:\n",
    "            print(f\"   ⚠️  No timestamps found - skipping time visualization\")\n",
    "            failed_visualizations.append((graph_basename, \"No timestamps\"))\n",
    "            continue\n",
    "\n",
    "        total_edges = G.number_of_edges()\n",
    "        if total_edges == 0:\n",
    "            print(f\"   ⚠️  No edges found - skipping entry visualization\")\n",
    "            failed_entry_visualizations.append((graph_basename, \"No edges\"))\n",
    "            continue\n",
    "        \n",
    "        # Load predictions if available\n",
    "        MALICIOUS_SPECS = []\n",
    "        has_predictions = False\n",
    "        \n",
    "        if os.path.exists(predictions_file):\n",
    "            predictions_df = pd.read_csv(predictions_file)\n",
    "            has_predictions = True\n",
    "            \n",
    "            # Handle different CSV formats\n",
    "            if 'LineID' in predictions_df.columns and 'Type' in predictions_df.columns and 'Score' in predictions_df.columns:\n",
    "                malicious_rows = predictions_df[predictions_df['Score'] >= 1.0]\n",
    "                for _, row in malicious_rows.iterrows():\n",
    "                    line_id = str(row['LineID'])\n",
    "                    prediction_type = row['Type'].lower()\n",
    "                    if prediction_type in ['src', 'dst']:\n",
    "                        MALICIOUS_SPECS.append((line_id, prediction_type))\n",
    "                    else:\n",
    "                        MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "            elif 'line_id' in predictions_df.columns:\n",
    "                malicious_rows = predictions_df[\n",
    "                    (predictions_df.get('prediction', 0) == 1) |\n",
    "                    (predictions_df.get('is_malicious', False) == True) |\n",
    "                    (predictions_df.get('label', 'benign').str.lower() == 'malicious')\n",
    "                ]\n",
    "                for _, row in malicious_rows.iterrows():\n",
    "                    line_id = str(row['line_id'])\n",
    "                    MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "        \n",
    "        print(f\"   Entry Count: {G.number_of_nodes()} nodes, {G.number_of_edges()} edges\")\n",
    "        print(f\"   Time range: {datetime.fromtimestamp(min_ts).strftime('%H:%M:%S')} to {datetime.fromtimestamp(max_ts).strftime('%H:%M:%S')}\")\n",
    "        print(f\"   Provided Malicious specs: {len(MALICIOUS_SPECS)}\")\n",
    "        \n",
    "        # Create interactive visualization\n",
    "        output_time_path = os.path.join(graph_dir, f\"interactive_time_{graph_basename}.html\")\n",
    "        output_entry_path = os.path.join(graph_dir, f\"interactive_entry_{graph_basename}.html\")\n",
    "        \n",
    "        # interactive_path = create_interactive_time_visualization(\n",
    "        #     G, edge_metadata, MALICIOUS_SPECS if MALICIOUS_SPECS else None, \n",
    "        #     output_path=output_path\n",
    "        # )\n",
    "        interactive_path = create_interactive_entry_visualization(\n",
    "            G, edge_metadata, MALICIOUS_SPECS if MALICIOUS_SPECS else None, \n",
    "            output_path=output_entry_path\n",
    "        )\n",
    "\n",
    "\n",
    "        # Calculate some time statistics\n",
    "        total_duration = max_ts - min_ts\n",
    "        seconds = total_duration\n",
    "        \n",
    "        successful_visualizations.append({\n",
    "            'graph': graph_basename,\n",
    "            'path': interactive_path,\n",
    "            'nodes': G.number_of_nodes(),\n",
    "            'edges': G.number_of_edges(),\n",
    "            'duration_seconds': seconds,\n",
    "            'malicious_specs': len(MALICIOUS_SPECS),\n",
    "            'has_predictions': has_predictions\n",
    "        })\n",
    "        \n",
    "        print(f\"   ✅ Interactive visualization created: {os.path.basename(interactive_path)}\")\n",
    "        print(f\"   📊 Duration: {seconds:.1f} seconds, REAPr: {'Yes' if MALICIOUS_SPECS else 'No'}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"   ❌ Error: {e}\")\n",
    "        failed_visualizations.append((graph_basename, str(e)))\n",
    "        continue\n",
    "\n",
    "print(f\"\\nSuccessful: {len(successful_visualizations)} | Failed: {len(failed_visualizations)}\")\n",
    "\n",
    "if successful_visualizations:\n",
    "    print(f\"\\nSUCCESSFUL VISUALIZATIONS:\")\n",
    "    print(f\"{'Graph':<35} {'Nodes':<8} {'Edges':<8} {'Duration':<12} {'REAPr':<8} {'File'}\")\n",
    "    print(\"-\" * 90)\n",
    "    \n",
    "    for viz in successful_visualizations:\n",
    "        duration_str = f\"{viz['duration_seconds']:.1f}s\"\n",
    "        reapr_str = \"Yes\" if viz['malicious_specs'] > 0 else \"No\"\n",
    "        filename = os.path.basename(viz['path'])\n",
    "        print(f\"{viz['graph']:<35} {viz['nodes']:<8} {viz['edges']:<8} {duration_str:<12} {reapr_str:<8} {filename}\")\n",
    "    \n",
    "    \n",
    "if failed_visualizations:\n",
    "    print(f\"\\n❌ FAILED VISUALIZATIONS:\")\n",
    "    for graph, reason in failed_visualizations:\n",
    "        print(f\"   {graph}: {reason}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d991198",
   "metadata": {},
   "source": [
    "# 攻擊序列模式視覺化 (Attack Sequence Pattern Visualization)\n",
    "\n",
    "此單元格專門針對特定檔案進行深度的攻擊序列模式分析和視覺化，展示系統如何識別和標記預定義的攻擊行為模式。\n",
    "\n",
    "## 核心功能 (Core Functions)\n",
    "\n",
    "### 1. 目標檔案設定 (Target File Configuration)\n",
    "- **目標檔案**: `cfb61005899996469ae3023796792ca5` (可調整)\n",
    "- 載入對應的圖形檔案 (`.pkl`) 和中繼資料 (`.json`)\n",
    "- 整合預測檔案中的惡意行為標記\n",
    "\n",
    "### 2. 序列群組偵測 (Sequence Group Detection)\n",
    "調用 `find_sequence_groups()` 函數執行智慧型模式比對：\n",
    "\n",
    "#### 參數設定 (Parameter Configuration)\n",
    "- **圖形物件 (G)**: NetworkX 圖形結構\n",
    "- **邊中繼資料 (edge_metadata)**: 包含操作詳細資訊\n",
    "- **攻擊模式 (ATTACK_SEQUENCE_PATTERNS)**: 預定義的攻擊序列\n",
    "- **最大間隔 (max_gap)**: 序列中允許的最大操作間隔\n",
    "- **目標檔案 (target_file)**: 用於偵錯和日誌記錄\n",
    "\n",
    "#### 偵測演算法 (Detection Algorithm)\n",
    "- 尋找符合預定義模式的操作序列\n",
    "- 計算模式匹配的置信度分數\n",
    "- 評估序列的完整性和順序性\n",
    "- 為每個找到的群組分配唯一的顏色標記\n",
    "\n",
    "### 3. 偵測結果分析 (Detection Results Analysis)\n",
    "系統提供詳細的偵測報告：\n",
    "\n",
    "#### 群組資訊 (Group Information)\n",
    "- **Group ID**: 群組唯一識別碼\n",
    "- **Pattern Name**: 匹配的攻擊模式名稱\n",
    "- **Description**: 模式的詳細描述\n",
    "- **Confidence Score**: 匹配置信度 (0.0-1.0)\n",
    "- **Edge Count**: 群組包含的邊數量\n",
    "- **Matched Operations**: 實際匹配的操作序列\n",
    "- **Expected Pattern**: 預期的操作模式\n",
    "- **Completeness**: 模式完整性評分\n",
    "\n",
    "### 4. 序列群組視覺化 (Sequence Group Visualization)\n",
    "調用 `create_sequence_grouped_visualization()` 生成專門的視覺化：\n",
    "\n",
    "#### 視覺化特色 (Visualization Features)\n",
    "- **顏色編碼**: 每種攻擊模式使用特定顏色\n",
    "- **群組高亮**: 屬於同一攻擊序列的邊使用相同顏色\n",
    "- **未分組邊**: 不屬於任何模式的邊以灰色顯示\n",
    "- **互動式介面**: 支援滑鼠懸停查看詳細資訊\n",
    "\n",
    "### 5. 覆蓋率分析 (Coverage Analysis)\n",
    "系統計算並報告模式覆蓋統計：\n",
    "\n",
    "#### 統計指標 (Statistical Metrics)\n",
    "- **群組化邊數**: 被歸類到攻擊模式的邊數量\n",
    "- **總邊數**: 圖形中的邊總數\n",
    "- **覆蓋率百分比**: 群組化邊占總邊數的比例\n",
    "- **未群組化邊數**: 未匹配任何模式的邊數量\n",
    "\n",
    "#### 模式分佈分析 (Pattern Distribution Analysis)\n",
    "- 每種攻擊模式匹配的邊數量\n",
    "- 各模式在整體圖形中的占比\n",
    "- 按邊數量排序的模式重要性排名\n",
    "\n",
    "### 6. 錯誤處理與偵錯 (Error Handling and Debugging)\n",
    "- 完整的例外處理和錯誤報告\n",
    "- 詳細的堆疊追蹤資訊\n",
    "- 檔案存在性驗證\n",
    "\n",
    "## 應用價值 (Application Value)\n",
    "此功能對網路安全分析特別重要：\n",
    "- **攻擊路徑視覺化**: 清楚展示攻擊者的行為序列\n",
    "- **模式識別**: 自動識別已知的攻擊技術\n",
    "- **異常偵測**: 突出顯示可疑的行為模式\n",
    "- **事件關聯**: 將相關的系統事件群組化\n",
    "\n",
    "這種基於模式的分析方法有助於安全分析師快速理解複雜的攻擊場景，並識別潛在的威脅行為。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fc582a1",
   "metadata": {},
   "source": [
    "# Generate visualization that colors the found patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c789935",
   "metadata": {},
   "source": [
    "# 單一檔案深度序列分析 (Single File Deep Sequence Analysis)\n",
    "\n",
    "此最終單元格展示如何對特定的日誌檔案執行深度的攻擊序列分析，結合了圖形載入、模式偵測、預測整合和專門的序列視覺化。\n",
    "\n",
    "## 分析流程 (Analysis Workflow)\n",
    "\n",
    "### 1. 檔案初始化 (File Initialization)\n",
    "- **目標檔案**: `cfb61005899996469ae3023796792ca5` (範例檔案)\n",
    "- 構建完整的檔案路徑結構\n",
    "- 驗證所有必要檔案的存在性\n",
    "\n",
    "### 2. 資料載入與整合 (Data Loading and Integration)\n",
    "\n",
    "#### 圖形資料載入 (Graph Data Loading)\n",
    "- 載入 pickle 格式的 NetworkX 圖形物件\n",
    "- 解析 JSON 格式的邊中繼資料\n",
    "- 重建邊識別碼到中繼資料的對應關係\n",
    "\n",
    "#### 邊中繼資料轉換 (Edge Metadata Conversion)\n",
    "將 JSON 中的字串形式邊識別碼轉換回元組格式：\n",
    "- 解析 `src→dst#key` 格式的邊識別碼\n",
    "- 重建 `(src, dst, key)` 元組作為字典鍵值\n",
    "- 確保資料結構與圖形物件的一致性\n",
    "\n",
    "#### 預測資料整合 (Prediction Data Integration)\n",
    "支援多種預測檔案格式的自動識別和載入：\n",
    "- **REAPr格式**: `LineID`, `Type`, `Score` 欄位\n",
    "- **通用格式**: `line_id`, `prediction`/`is_malicious`/`label` 欄位\n",
    "- 自動篩選高分或標記為惡意的條目\n",
    "\n",
    "### 3. 智慧序列偵測 (Intelligent Sequence Detection)\n",
    "調用 `find_sequence_groups()` 執行進階的攻擊模式分析：\n",
    "\n",
    "#### 偵測參數 (Detection Parameters)\n",
    "- **最大間隔 (max_gap)**: 1 (序列中允許的操作間隔)\n",
    "- **模式庫**: 使用預定義的 `ATTACK_SEQUENCE_PATTERNS`\n",
    "- **目標檔案**: 用於特定的偵錯和最佳化\n",
    "\n",
    "#### 偵測結果展示 (Detection Results Display)\n",
    "提供前5個偵測到的群組的詳細資訊：\n",
    "- 群組識別碼和模式名稱\n",
    "- 置信度分數和完整性評估\n",
    "- 實際匹配的操作與預期模式的對比\n",
    "- 視覺化顏色分配\n",
    "\n",
    "### 4. 進階視覺化生成 (Advanced Visualization Generation)\n",
    "調用 `create_sequence_grouped_visualization()` 創建專門的序列分析視覺化：\n",
    "\n",
    "#### 視覺化特色 (Visualization Features)\n",
    "- **模式顏色編碼**: 每種攻擊模式使用專屬顏色\n",
    "- **序列群組標記**: 相同序列的邊使用統一顏色\n",
    "- **惡意行為高亮**: 整合預測資料的惡意標記\n",
    "- **未分類邊處理**: 灰色顯示不屬於任何模式的邊\n",
    "\n",
    "### 5. 統計分析與報告 (Statistical Analysis and Reporting)\n",
    "\n",
    "#### 覆蓋率統計 (Coverage Statistics)\n",
    "- **群組化覆蓋率**: 計算被歸類到攻擊模式的邊的百分比\n",
    "- **未群組化邊數**: 統計未匹配任何模式的邊\n",
    "- **整體分析效果**: 評估模式偵測的成效\n",
    "\n",
    "#### 模式分佈報告 (Pattern Distribution Report)\n",
    "- **按模式統計**: 每種攻擊模式匹配的邊數量\n",
    "- **重要性排序**: 按邊數量排序的模式重要性\n",
    "- **百分比分析**: 各模式在整體活動中的占比\n",
    "\n",
    "### 6. 異常處理與驗證 (Exception Handling and Validation)\n",
    "- 完整的錯誤捕捉和報告機制\n",
    "- 檔案存在性的多重驗證\n",
    "- 詳細的堆疊追蹤資訊用於偵錯\n",
    "\n",
    "## 分析價值 (Analytical Value)\n",
    "\n",
    "### 安全分析應用 (Security Analysis Applications)\n",
    "- **威脅狩獵 (Threat Hunting)**: 識別複雜的攻擊鏈\n",
    "- **事件回應 (Incident Response)**: 快速理解攻擊序列\n",
    "- **模式學習 (Pattern Learning)**: 發現新的攻擊行為模式\n",
    "- **風險評估 (Risk Assessment)**: 量化攻擊活動的嚴重程度\n",
    "\n",
    "### 視覺化優勢 (Visualization Advantages)\n",
    "- **直觀理解**: 複雜的日誌資料轉化為視覺化圖形\n",
    "- **互動探索**: 支援動態篩選和詳細查看\n",
    "- **模式識別**: 自動標記和分類攻擊行為\n",
    "- **關聯分析**: 展示事件之間的時間和邏輯關係\n",
    "\n",
    "這個深度分析功能為網路安全專家提供了強大的工具，能夠從大量的系統日誌中快速識別和分析潛在的安全威脅。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077f75d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Sequence Grouping\n",
    "\n",
    "target_file = \"cfb61005899996469ae3023796792ca5\"  # Change this to your target file\n",
    "graph_file = os.path.join(GRAPH_PATH, target_file, f\"{target_file}.pkl\")\n",
    "\n",
    "\n",
    "graph_basename = os.path.basename(os.path.dirname(graph_file))\n",
    "graph_dir = os.path.dirname(graph_file)\n",
    "edge_metadata_file = os.path.join(graph_dir, f\"{graph_basename}_edge_metadata.json\")\n",
    "predictions_file = f\"Caldera_Ability_Predictions/{graph_basename}.csv\"\n",
    "\n",
    "if os.path.exists(graph_file) and os.path.exists(edge_metadata_file):\n",
    "    \n",
    "    # Load the graph and metadata\n",
    "    G = pickle.load(open(graph_file, \"rb\"))\n",
    "    with open(edge_metadata_file, \"r\") as fp:\n",
    "        edge_metadata_json = json.load(fp)\n",
    "    \n",
    "    # Convert JSON edge metadata back to the expected format\n",
    "    edge_metadata = {}\n",
    "    for edge_id, data in edge_metadata_json.items():\n",
    "        parts = edge_id.split('→')\n",
    "        if len(parts) == 2:\n",
    "            src = parts[0]\n",
    "            dst_key = parts[1].split('#')\n",
    "            if len(dst_key) == 2:\n",
    "                dst = dst_key[0]\n",
    "                key = int(dst_key[1]) if dst_key[1].isdigit() else dst_key[1]\n",
    "                edge_metadata[(src, dst, key)] = data\n",
    "    \n",
    "    # Load predictions if available\n",
    "    MALICIOUS_SPECS = []\n",
    "    if os.path.exists(predictions_file):\n",
    "        predictions_df = pd.read_csv(predictions_file)\n",
    "        \n",
    "        if 'LineID' in predictions_df.columns and 'Type' in predictions_df.columns and 'Score' in predictions_df.columns:\n",
    "            malicious_rows = predictions_df[predictions_df['Score'] >= 1.0]\n",
    "            for _, row in malicious_rows.iterrows():\n",
    "                line_id = str(row['LineID'])\n",
    "                prediction_type = row['Type'].lower()\n",
    "                if prediction_type in ['src', 'dst']:\n",
    "                    MALICIOUS_SPECS.append((line_id, prediction_type))\n",
    "                else:\n",
    "                    MALICIOUS_SPECS.append((line_id, \"both\"))\n",
    "\n",
    "    \n",
    "    sequence_groups = find_sequence_groups(\n",
    "        G, edge_metadata, ATTACK_SEQUENCE_PATTERNS, \n",
    "        max_gap=1, target_file=target_file\n",
    "    )\n",
    "    \n",
    "    # Show details of detected groups\n",
    "    if sequence_groups:\n",
    "        print(f\"\\n📋 DETECTED SEQUENCE GROUPS:\")\n",
    "        for group_id, group_info in list(sequence_groups.items())[:5]:  # Show first 5 groups\n",
    "            print(f\"\\n   Group {group_id}: {group_info['pattern'].name}\")\n",
    "            print(f\"      Description: {group_info['pattern'].description}\")\n",
    "            print(f\"      Confidence: {group_info['confidence']:.2f}\")\n",
    "            print(f\"      Edges: {len(group_info['edges'])}\")\n",
    "            print(f\"      Operations: {', '.join(group_info['matched_operations'][:3])}{'...' if len(group_info['matched_operations']) > 3 else ''}\")\n",
    "            print(f\"      Expected pattern: {', '.join(group_info['pattern'].operations)}\")\n",
    "            print(f\"      Strict order: {group_info['pattern'].strict_order}\")\n",
    "            print(f\"      Completeness: {group_info.get('completeness', 0):.2f}\")\n",
    "            print(f\"      Color: {group_info['pattern'].color}\")\n",
    "\n",
    "    try:\n",
    "        output_path = os.path.join(graph_dir, f\"sequence_grouped_{graph_basename}.html\")\n",
    "        viz_path, groups = create_sequence_grouped_visualization(\n",
    "            G, edge_metadata, MALICIOUS_SPECS, ATTACK_SEQUENCE_PATTERNS, output_path, sequence_groups\n",
    "        )\n",
    "        \n",
    "        \n",
    "        # Summary of what was found\n",
    "        if groups:\n",
    "            total_grouped_edges = sum(len(g['edges']) for g in groups.values())\n",
    "            total_edges = G.number_of_edges()\n",
    "            coverage = (total_grouped_edges / total_edges) * 100\n",
    "            \n",
    "            print(f\"\\n📊 Sequence Coverage:\")\n",
    "            print(f\"   Grouped edges: {total_grouped_edges}/{total_edges} ({coverage:.1f}%)\")\n",
    "            print(f\"   Ungrouped edges: {total_edges - total_grouped_edges} (shown in gray)\")\n",
    "            \n",
    "            # Show pattern distribution\n",
    "            pattern_counts = defaultdict(int)\n",
    "            for group_info in groups.values():\n",
    "                pattern_counts[group_info['pattern'].name] += len(group_info['edges'])\n",
    "            \n",
    "            print(f\"\\n🎯 Pattern Distribution:\")\n",
    "            for pattern_name, edge_count in sorted(pattern_counts.items(), key=lambda x: x[1], reverse=True):\n",
    "                percentage = (edge_count / total_edges) * 100\n",
    "                print(f\"   {pattern_name}: {edge_count} edges ({percentage:.1f}%)\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error creating sequence visualization: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "else:\n",
    "    print(f\"❌ Required files not found for {target_file}\")\n",
    "    print(f\"   Graph file: {graph_file}\")\n",
    "    print(f\"   Metadata file: {edge_metadata_file}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.10.11)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
